[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; loan_data &lt;- read_csv(here::here(\"dataset\", \"loan_refusal.csv\"))\n\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n&gt; loan_data_clean &lt;- pivot_longer(loan_data, 2:5, names_to = \"group\", \n+     values_to = \"refusal_rate\")\n\n&gt; write_rds(loan_data_clean, file = here::here(\"dataset\", \n+     \"loan_refusal_clean.rds\"))\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\nSean Fung, Zihao Guo, Siqi Chen\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 6\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 5\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nSean Fung, Zihao Guo, Yawen Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 4\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post 3\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-10-31-blog-post-4/blog-post-4.html",
    "href": "posts/2024-10-31-blog-post-4/blog-post-4.html",
    "title": "blog post 4",
    "section": "",
    "text": "library(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project\n\ndata &lt;- readRDS(here(\"filtered_data_2.rds\"))\n\nfiltered_data &lt;- subset(data, Descent_Description != \"Unknown\" & Descent_Description != \"Other\")\n\n# Create a contingency table for the filtered data\ncrime_table_filtered &lt;- table(filtered_data$Descent_Description, filtered_data$Crm.Cd.Desc)\n\nchi_test_filtered &lt;- chisq.test(crime_table_filtered)\n\nWarning in chisq.test(crime_table_filtered): Chi-squared approximation may be\nincorrect\n\nprint(chi_test_filtered)\n\n\n    Pearson's Chi-squared test\n\ndata:  crime_table_filtered\nX-squared = 2363.8, df = 1222, p-value &lt; 2.2e-16\n\n# Extract standardized residuals\nstandardized_residuals_filtered &lt;- chi_test_filtered$stdres\n\ntop_crimes &lt;- names(sort(colSums(crime_table_filtered), decreasing = TRUE))[1:20]\n\n# Filter the contingency table to only include top crime types\ncrime_table_top &lt;- crime_table_filtered[, top_crimes]\n\nchi_test_top &lt;- chisq.test(crime_table_top)\n\nWarning in chisq.test(crime_table_top): Chi-squared approximation may be\nincorrect\n\nstandardized_residuals_top &lt;- chi_test_top$stdres\n\n\nresiduals_top_df &lt;- as.data.frame(as.table(standardized_residuals_top))\ncolnames(residuals_top_df) &lt;- c(\"Descent\", \"Crime_Type\", \"Residual\")\n\n\n\nggplot(residuals_top_df, aes(x = Crime_Type, y = Descent, fill = Residual)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  labs(title = \"Standardized Residuals of Descent and Top Crime Types\",\n       x = \"Crime Type\", y = \"Descent Category\", fill = \"Residual\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nI wanted to ask the question of whether every crime was correlated to descent. Therefore, I conducted a chi-squared test and wanted to see how each race’s residual was in comparison to the top 10 crimes (all crimes would be too much). - We can see that “White” had a high residual in the burglary category and higher residuals in some other categories as well. We also see lower than average residuals in the aggrevated assault and simple assault categories. - As for Hispanic/Mexican, even though they represent such a high amount of crimes (as seen in last blog post), they are moderately around what is expected with the exception of the “Burglary” and “Theft (under $950) categories. - Other Asian slightly higher in”Burglary” - “Black” lower in “theft from motor vehicle”. - “Filipino” higher in “theft from motor vehicle”. - “Chinese” higher in “theft - grand except guns or livestock”. - Everything else is mostly the same hovering around the expected with small fluctuations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncrime_hour_descent &lt;- data %&gt;%\n  group_by(Hour, Crm.Cd.Desc, Descent_Description) %&gt;%\n  summarise(Count = n(), .groups = \"drop\") %&gt;%\n  filter(Count &gt; 0)\n\nggplot(crime_hour_descent, aes(x = Hour, y = Descent_Description, fill = Count)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkred\", name = \"Crime Count\") + \n  labs(title = \"Heatmap of Crime Counts by Hour and Descent\",\n       x = \"Hour of Day\",\n       y = \"Descent Category\")\n\n\n\n\n\n\n\n\nI make this plot to visualize how crime frequency varies for each descent category by time of the day. I use heatmap since it provides a direct comparison across different times and descents. This plot clearly shows that certain racial categories, such as White, Hispanic/Latin/Mexican, and Black, experience higher crime rates compared to racial groups like Cambodian and Laotian, potentially due to smaller sample sizes or data collection biases.\nThe general trend across most categories indicates a peak in crime rates from late evening to early morning (00:00 to 05:00), which is the same as what we expected. However, an additional peak occurs during daytime (10:00 to 15:00), which is unusual and needs further investigation.\n\nlibrary(dplyr)\nlibrary(here)\ncrime_data &lt;- readRDS(here(\"filtered_data_2.rds\")) %&gt;%\n  mutate(across(c(AREA.NAME, Crm.Cd.Desc, Descent_Description), ~ trimws(tolower(.))))\ncrime_area_descent_summary &lt;- crime_data %&gt;%\n  count(AREA.NAME, Crm.Cd.Desc, Descent_Description, name = \"Crime_Count\")\n\n# Get top 10 areas and crimes\ntop_10_areas &lt;- crime_area_descent_summary %&gt;%\n  count(AREA.NAME, wt = Crime_Count, sort = TRUE) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(AREA.NAME)\ntop_10_crimes &lt;- crime_area_descent_summary %&gt;%\n  count(Crm.Cd.Desc, wt = Crime_Count, sort = TRUE) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(Crm.Cd.Desc)\n\nfiltered_data &lt;- crime_area_descent_summary %&gt;%\n  filter(AREA.NAME %in% top_10_areas, Crm.Cd.Desc %in% top_10_crimes)\n\n# Chi-square test for Crime Type vs. Area\ncrime_type_area&lt;- xtabs(Crime_Count ~ Crm.Cd.Desc + AREA.NAME, data = filtered_data)\nchi_test_crime_area &lt;- chisq.test(crime_type_area)\nprint(\"Chi-Square Test Result: Crime Type vs. Area\")\n\n[1] \"Chi-Square Test Result: Crime Type vs. Area\"\n\nprint(chi_test_crime_area)\n\n\n    Pearson's Chi-squared test\n\ndata:  crime_type_area\nX-squared = 451.13, df = 81, p-value &lt; 2.2e-16\n\n# Chi-square test for Descent Group vs. Area\ndescent_area &lt;- xtabs(Crime_Count ~ Descent_Description + AREA.NAME, data = filtered_data)\nchi_test_descent_area &lt;- chisq.test(descent_area)\n\nWarning in chisq.test(descent_area): Chi-squared approximation may be incorrect\n\nprint(\"Chi-Square Test Result: Descent Group vs. Area\")\n\n[1] \"Chi-Square Test Result: Descent Group vs. Area\"\n\nprint(chi_test_descent_area)\n\n\n    Pearson's Chi-squared test\n\ndata:  descent_area\nX-squared = 1051.2, df = 135, p-value &lt; 2.2e-16\n\n\nboth of the chi-square tests have low p-value, indicating that the variables are statistically associated. With the chi-sqaure test on crime type - descent group, I wanted to understand the distribution across these three variables at once.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plot\nggplot(filtered_data, aes(x = reorder(Crm.Cd.Desc, -Crime_Count), y = Crime_Count, fill = AREA.NAME)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Descent_Description, ncol = 3) +\n  labs(\n    title = \"Most Prevalent Area for Top Crimes by Descent Group (Top 10 Areas and Crimes)\",\n    x = \"Crime Type\",\n    y = \"Crime Count\",\n    fill = \"Area\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 6),\n    strip.text = element_text(size = 10),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nThis graph shows the distributions across crime types, geographic areas, and descent groups within the top 10 most frequent crime types and top 10 areas(it’s actually 11 areas in the graph: ‘hollywood’ and ’ n hollywood’ are counted as separate entries) with the highest crime counts. Here are some key patterns from the graph: noted that some descent groups, like Black, Hispanic/Latin/Mexican, and White, experienced a wider range of crime types across multiple areas, whereas others had more limited exposure. Certain areas, such as 77th Street, Central, and Southwest, consistently showed high counts across multiple crime types, showing that they are hopspts for various types of crime. Last but not least, vehicle theft, simple assault, and petty theft were widespread acroos areas and descent groups."
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html",
    "title": "blog post 5",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidycensus)\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n# Load the crime data and convert to spatial format\ndata &lt;- readRDS(here(\"filtered_data_2.rds\"))\ncrime_sf &lt;- st_as_sf(data, coords = c(\"LON\", \"LAT\"), crs = 4326)\n\n# Load neighborhood boundaries\nneighborhoods &lt;- st_read(here(\"LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp\"))\n\nReading layer `8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu' from data source `/Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project/LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 6359592 ymin: 1715035 xmax: 6514633 ymax: 1945515\nProjected CRS: NAD83 / California zone 5 (ftUS)\n\nneighborhoods &lt;- st_transform(neighborhoods, st_crs(crime_sf))\n\n# Perform the spatial join to link crime incidents with neighborhoods\ncrime_with_neighborhoods &lt;- st_join(crime_sf, neighborhoods, join = st_within)\n\n# Load and join median income data\neconomic_data &lt;- get_acs(\n    geography = \"block group\",\n    variables = c(median_income = \"B19013_001\"),\n    state = \"CA\",\n    county = \"Los Angeles\",\n    year = 2020,\n    geometry = TRUE\n)\n\nGetting data from the 2016-2020 5-year ACS\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\neconomic_data &lt;- st_transform(economic_data, st_crs(crime_with_neighborhoods))\ncrime_with_income &lt;- st_join(crime_with_neighborhoods, economic_data, join = st_within)\n\n# Summarize crime data by neighborhood, with average income and ensuring valid geometry\ncrime_summary &lt;- crime_with_income %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarize(\n    total_crimes = n(),\n    avg_income = mean(estimate, na.rm = TRUE),\n    geometry = st_union(geometry)\n  ) %&gt;%\n  st_as_sf()\n\n# Filter out rows with NA values in avg_income\ncrime_summary &lt;- crime_summary %&gt;% filter(!is.na(avg_income))\n# Pull median income data for Los Angeles\nincome_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(median_income = \"B19013_001\"),\n  state = \"CA\",\n  county = \"Los Angeles\",\n  year = 2020,\n  geometry = TRUE\n)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Prepare the data for mapping (ensure CRS consistency)\nincome_data &lt;- st_transform(income_data, crs = st_crs(crime_summary))\n\n# Join income data with crime data (assuming `crime_summary` has neighborhood geometries)\ncrime_income_map &lt;- st_join(crime_summary, income_data, join = st_within)\n\n# Filter out areas with missing income data (if needed)\ncrime_income_map &lt;- crime_income_map %&gt;% filter(!is.na(estimate))\n\n# Ensure geometries are valid and filter out empty or invalid geometries\n# Filter out geometries with coordinates at (0, 0) or zero degrees west (longitude = 0)\ncrime_income_map &lt;- crime_income_map %&gt;%\n  filter(!st_is_empty(geometry)) %&gt;%  # Ensure geometry is not empty\n  filter(!sapply(geometry, function(geom) {\n    coords &lt;- st_coordinates(geom)\n    return(any(coords[,1] == 0 & coords[,2] == 0) || any(coords[,1] == 0) || any(coords[,2] == 0))\n  }))\n\nggplot() +\n  geom_sf(data = income_data, aes(fill = estimate), color = NA) +  # Median income layer\n  geom_sf(data = crime_summary, aes(color = avg_income), size = 2, alpha = 0.7) +  # Crime data layer\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  labs(\n    title = \"Median Income and Crime Data by Neighborhood in Los Angeles\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Adjust these values to focus on the Los Angeles area\n    ylim = c(33, 35)       # Adjust these values to focus on the Los Angeles area\n  )"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#we-had-combine-the-temp-data-and-data-that-we-are-using-by-using-left-join",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#we-had-combine-the-temp-data-and-data-that-we-are-using-by-using-left-join",
    "title": "blog post 5",
    "section": "We had combine the temp data and data that we are using by using left join",
    "text": "We had combine the temp data and data that we are using by using left join\n\nlatemp &lt;- readRDS(here(\"latemp.rds\"))\nlatemp &lt;- latemp %&gt;%\n  mutate(DATE = as.Date(DATE)) %&gt;%\n  rename(DATE.OCC = DATE)\nlatemp_unique &lt;- latemp %&gt;%\n  distinct(DATE.OCC, .keep_all = TRUE)\ndata_with_temp &lt;- data %&gt;%\n  left_join(latemp_unique, by = \"DATE.OCC\")"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#find-the-top-20-temperture-that-mostly-to-occur-crime",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#find-the-top-20-temperture-that-mostly-to-occur-crime",
    "title": "blog post 5",
    "section": "Find the top 20 temperture that mostly to occur crime",
    "text": "Find the top 20 temperture that mostly to occur crime\n\ntop_20_temps &lt;- data_with_temp %&gt;%\n  count(TAVG, sort = TRUE) %&gt;%\n  top_n(20, n)\n\ntop_20_temps\n\n# A tibble: 20 × 2\n    TAVG     n\n   &lt;int&gt; &lt;int&gt;\n 1    60   754\n 2    58   737\n 3    63   660\n 4    67   654\n 5    65   589\n 6    59   584\n 7    62   584\n 8    69   583\n 9    61   564\n10    71   527\n11    66   510\n12    64   498\n13    70   483\n14    72   473\n15    74   461\n16    55   435\n17    57   435\n18    56   431\n19    68   370\n20    73   361"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#ggplot-for-temp-vs-crime-occurs",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#ggplot-for-temp-vs-crime-occurs",
    "title": "blog post 5",
    "section": "ggplot for temp vs crime occurs",
    "text": "ggplot for temp vs crime occurs\n\nggplot(data_with_temp, aes(x = TAVG)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Crime Count by Temperature\",\n       x = \"Average Temperature (TAVG)\",\n       y = \"Crime Count\") +\n  theme_minimal()\n\nWarning: Removed 29 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe histogram shows that crime counts are higher in the temperature range of approximately 55°F to 75°F. Within this range, the frequency of crimes appears to be fairly consistent, with a peak around 60°F to 70°F. As temperatures exceed 75°F, there is a noticeable decline in crime counts. This suggests that extremely warm days may be associated with lower crime activity. Similarly, at lower temperatures (below 50°F), there are also fewer crime incidents. This could be due to people staying indoors more on colder days, leading to less social interaction and possibly fewer opportunities for crime."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "This dataset reflects incidents of crime in the City of Los Angeles dating back to 2020 which is collected and sourced by the Los Angeles Police Department. The LAPD collects this data as part of routine public safety operations in order to better track crime patterns, allocating resources, and for transparency purposes, ensuring that the public has access to crime data. While looking through the dataset, there are some issues with collecting the data. Since the LAPD records the crime data through reports, there can be several inconsistencies with how crime reports with different officers or stations taking in data as well as failing to account for unreported crime. This would lead to a dataset that doesn’t capture the full picture of crime in LA. With the dataset collecting crime reported from 2020 to the present, there are multiple reasons to think that the sample is biased pertaining to crime. Crime data can be biased due to socioeconomic and geographic factors as crime in lower-income neighborhoods are policed more often and more likely report certain kinds of crime which would skew the data in some ways. Additionally, less policed neighborhoods would have underreported crime data. This data can be used to shape policy in law enforcement to better allocate police resources to prevent crime and be used for research to study socioeconomic facts that play into crime in LA. There has been other research on the same data where they look into more niche subtopics of crime such as domestic violence or crime involving homeless people."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#background-information-oscar",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#background-information-oscar",
    "title": "Blog Post 2",
    "section": "",
    "text": "This dataset reflects incidents of crime in the City of Los Angeles dating back to 2020 which is collected and sourced by the Los Angeles Police Department. The LAPD collects this data as part of routine public safety operations in order to better track crime patterns, allocating resources, and for transparency purposes, ensuring that the public has access to crime data. While looking through the dataset, there are some issues with collecting the data. Since the LAPD records the crime data through reports, there can be several inconsistencies with how crime reports with different officers or stations taking in data as well as failing to account for unreported crime. This would lead to a dataset that doesn’t capture the full picture of crime in LA. With the dataset collecting crime reported from 2020 to the present, there are multiple reasons to think that the sample is biased pertaining to crime. Crime data can be biased due to socioeconomic and geographic factors as crime in lower-income neighborhoods are policed more often and more likely report certain kinds of crime which would skew the data in some ways. Additionally, less policed neighborhoods would have underreported crime data. This data can be used to shape policy in law enforcement to better allocate police resources to prevent crime and be used for research to study socioeconomic facts that play into crime in LA. There has been other research on the same data where they look into more niche subtopics of crime such as domestic violence or crime involving homeless people."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-1-data-reduction",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-1-data-reduction",
    "title": "Blog Post 2",
    "section": "Step 1: Data Reduction",
    "text": "Step 1: Data Reduction\nSince the original dataset was too large, I used a separate R project to filter and reduce the number of rows in the CSV file. After processing, the data was saved as an RDS file to make it easier to work with. Below is the code I used:\n# Convert the date column to year format\ndata$year &lt;- format(as.Date(data$DATE.OCC, format = \"%m/%d/%Y\"), \"%Y\")\ndata$year &lt;- as.numeric(data$year)  # Convert year to numeric\n\n# Filter for the years 2020–2024\nfiltered_years &lt;- data %&gt;%\n  filter(year %in% 2020:2024)\n\n# Split by year and sample up to 2,500 rows per year\nsampled_data &lt;- filtered_years %&gt;%\n  group_split(year) %&gt;%\n  map_df(~ slice_sample(.x, n = min(2500, nrow(.x))))\n\n# View the sampled data\nhead(sampled_data)\n\n# Save the filtered data as an RDS file\nsaveRDS(sampled_data, \"filtered_data.rds\")"
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-2-data-import-for-group-collaboration",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-2-data-import-for-group-collaboration",
    "title": "Blog Post 2",
    "section": "Step 2: Data Import for Group Collaboration",
    "text": "Step 2: Data Import for Group Collaboration\nAfter cleaning and sampling the data, I imported the RDS file into the final project so that all group members could access it easily.\n# Example of loading the RDS file\nsampled_data &lt;- readRDS(\"filtered_data.rds\")\nThis approach ensures that our dataset is manageable and ready for further analysis, while also maintaining the integrity of the original data."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-loading-and-exploration-zihao-and-siqi",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-loading-and-exploration-zihao-and-siqi",
    "title": "Blog Post 2",
    "section": "Data Loading and exploration (Zihao and Siqi)",
    "text": "Data Loading and exploration (Zihao and Siqi)\nAfter loading RDS file, we found one interesting facts that we may need to clean out some meaningless value in dataset.\n#ggplot(data,mapping = aes(x=Vict.Age))+geom_histogram(stat=\"count\")\nA lot of officers did not enter the victim’s age, so we can’t do some exploration of the age of victim would most likely to be the target.\nWe also explore the area that would have the most crime.\n#ggplot(data,mapping = aes(x=AREA))+geom_histogram(stat=\"count\")\nIt shows that the top 3 areas are Area 1(Central), Area 12(77th Street), and Area 14(Pacific).\nLast, we also find the count of victims race in the data.\n#ggplot(data,mapping = aes(x=Vict.Descent))+geom_histogram(stat=\"count\")\nIt shows that H (Hispanic/Latin/Mexican)is the most targeted victims. The other few big victims group would be B(Black), O(Other), W(White), and X(Unknown).\nOne thing to be notice is it has the same issue with age that officers did not record 2000 cases’ victims race."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-for-equity-yawen",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-for-equity-yawen",
    "title": "Blog Post 2",
    "section": "Data for Equity (Yawen)",
    "text": "Data for Equity (Yawen)\nTransparency Transparency involves being open about the context of the data and the decisions made during analysis. This includes being clear about how the crime data was collected, what factors are included, and any potential biases present. For example, one potential bias could be over-policing or under-policing in certain areas, which may lead to negative analysis if not acknowledged. Over-policing in a neighborhood could result in a higher number of reported crimes, not necessarily because there is more crime, but due to increased law enforcement presence. Conversely, under-policing in other areas may result in under-reporting, giving a false sense of safety. Both situations can harm residents by reinforcing stereotypes by labeling certain neighborhoods as “high crime”. Adhering to transparency would also involve disclosing any missing data, clarifying that the dataset only reflects reported crimes, and possibly under-reports certain types of crime. Justice Justice is the commitment to the fair distribution of burdens and benefits among people. In our dataset, this means ensuring that data collection and analysis do not harm individuals or communities. Crime data, if not handled carefully, can expose people to risks such as privacy and safety. One way to minimize these risks is to ensure that personally identifiable information, such as names, social security numbers or dates of birth is not included in the dataset. Additionally, the inclusion of specific crime locations in this dataset, could unfairly label certain areas as “high crime”, producing negative perceptions. To promote justice, LAPD could engage with the community residents by holding listening sessions to learn what data the community thinks are relevant to improve their lives. Potential limitations One limitation of the analysis could be the risk of reinforcing stereotypes. Crime data, particularly when aggregated, might suggest that certain neighborhoods are inherently more “criminal” or that specific racial groups are prone to crime, without considering systemic factors such as poverty."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html",
    "title": "Blog post 3",
    "section": "",
    "text": "The following code blocks can be found in the scripts file under Sean test scripts for a complete R file for testing. This will be higher level descriptions."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#background-information",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#background-information",
    "title": "Blog post 3",
    "section": "",
    "text": "The following code blocks can be found in the scripts file under Sean test scripts for a complete R file for testing. This will be higher level descriptions."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#load-required-libraries",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#load-required-libraries",
    "title": "Blog post 3",
    "section": "1. Load Required Libraries",
    "text": "1. Load Required Libraries\n\n# Load required libraries\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project"
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#clean-dataset-for-readability-and-ease-of-use",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#clean-dataset-for-readability-and-ease-of-use",
    "title": "Blog post 3",
    "section": "2. Clean Dataset for Readability and Ease of Use",
    "text": "2. Clean Dataset for Readability and Ease of Use\n\ndata &lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndata &lt;- readRDS(\"filtered_data_copy.rds\")\n\n\n# Create a tibble with the mapping of descent codes\ndescent_codes &lt;- tribble(\n  ~Descent_Code, ~Descent_Description,\n  \"A\", \"Other Asian\",\n  \"B\", \"Black\",\n  \"C\", \"Chinese\",\n  \"D\", \"Cambodian\",\n  \"F\", \"Filipino\",\n  \"G\", \"Guamanian\",\n  \"H\", \"Hispanic/Latin/Mexican\",\n  \"I\", \"American Indian/Alaskan Native\",\n  \"J\", \"Japanese\",\n  \"K\", \"Korean\",\n  \"L\", \"Laotian\",\n  \"O\", \"Other\",\n  \"P\", \"Pacific Islander\",\n  \"S\", \"Samoan\",\n  \"U\", \"Hawaiian\",\n  \"V\", \"Vietnamese\",\n  \"W\", \"White\",\n  \"X\", \"Unknown\",\n  \"Z\", \"Asian Indian\"\n)\n\n# Join the tibble with the main dataset by Vict.Descent\ndata_clean &lt;- data %&gt;%\n  left_join(descent_codes, by = c(\"Vict.Descent\" = \"Descent_Code\"))\n\n# Replace unmatched or missing descriptions with \"Unknown\"\ndata_clean$Descent_Description[is.na(data_clean$Descent_Description)] &lt;- \"Unknown\"\n\n# Drop irrelevant columns with many NAs\ndata_clean &lt;- data_clean[, !(names(data_clean) %in% c(\"Crm.Cd.2\", \"Crm.Cd.3\", \"Crm.Cd.4\", \"Weapon.Used.Cd\", \"Weapon.Desc\", \"Cross.Street\"))]\ndata_clean$DATE.OCC &lt;- as.POSIXct(data_clean$DATE.OCC, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n# Extract the 'month' column in \"YYYY-MM\" format\ndata_clean$month &lt;- format(data_clean$DATE.OCC, \"%Y-%m\")\n\n# Check the cleaned data\nnrow(data_clean)\n\n[1] 12500\n\nstr(data_clean)\n\ntibble [12,500 × 25] (S3: tbl_df/tbl/data.frame)\n $ DR_NO              : int [1:12500] 200604124 200218223 210105387 201314683 201610483 200907309 200905047 200111368 201508876 201111487 ...\n $ Date.Rptd          : chr [1:12500] \"01/03/2020 12:00:00 AM\" \"12/05/2020 12:00:00 AM\" \"02/02/2021 12:00:00 AM\" \"07/29/2020 12:00:00 AM\" ...\n $ DATE.OCC           : POSIXct[1:12500], format: \"2020-01-03\" \"2020-12-05\" ...\n $ TIME.OCC           : int [1:12500] 400 100 1550 1035 1455 1220 1230 2007 417 1235 ...\n $ AREA               : int [1:12500] 6 2 1 13 16 9 9 1 15 11 ...\n $ AREA.NAME          : chr [1:12500] \"Hollywood\" \"Rampart\" \"Central\" \"Newton\" ...\n $ Rpt.Dist.No        : int [1:12500] 648 235 152 1383 1695 994 915 159 1549 1115 ...\n $ Part.1.2           : int [1:12500] 1 1 2 2 2 1 2 2 1 2 ...\n $ Crm.Cd             : int [1:12500] 310 510 649 740 354 310 745 940 310 662 ...\n $ Crm.Cd.Desc        : chr [1:12500] \"BURGLARY\" \"VEHICLE - STOLEN\" \"DOCUMENT FORGERY / STOLEN FELONY\" \"VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)\" ...\n $ Mocodes            : chr [1:12500] \"0325 0601 1601 1606\" \"\" \"1214 0923\" \"0329 1307\" ...\n $ Vict.Age           : int [1:12500] 0 0 0 0 26 48 57 37 0 60 ...\n $ Vict.Sex           : chr [1:12500] \"X\" \"\" \"X\" \"M\" ...\n $ Vict.Descent       : chr [1:12500] \"X\" \"\" \"X\" \"W\" ...\n $ Premis.Cd          : int [1:12500] 210 101 602 122 501 501 157 502 222 501 ...\n $ Premis.Desc        : chr [1:12500] \"RESTAURANT/FAST FOOD\" \"STREET\" \"BANK\" \"VEHICLE, PASSENGER/TRUCK\" ...\n $ Status             : chr [1:12500] \"IC\" \"IC\" \"IC\" \"IC\" ...\n $ Status.Desc        : chr [1:12500] \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" ...\n $ Crm.Cd.1           : int [1:12500] 310 510 649 740 354 310 745 940 310 662 ...\n $ LOCATION           : chr [1:12500] \"5400 W  SUNSET                       BL\" \"2200 W  TEMPLE                       ST\" \"800    WILSHIRE                     BL\" \"6200 S  BROADWAY\" ...\n $ LAT                : num [1:12500] 34.1 34.1 34 34 34.2 ...\n $ LON                : num [1:12500] -118 -118 -118 -118 -118 ...\n $ year               : num [1:12500] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n $ Descent_Description: chr [1:12500] \"Unknown\" \"Unknown\" \"Unknown\" \"White\" ...\n $ month              : chr [1:12500] \"2020-01\" \"2020-12\" \"2020-10\" \"2020-07\" ...\n\n\nThis was done in an effort to make the data more readable and cleaner. It gets rid of columns that had a lot of N/A’s in the column and changes the date column to remove the 12:00 AM that was previously there and replacing it with a “DATE OCC” column that was not initially there to make the data easier to change.\nIt also creates another column that takes the Vict.Descent column and uses the codes from a tibble to create another column called “Descent_Code” in order to get a more readable information. Then, I checked the data."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#data-visualization",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#data-visualization",
    "title": "Blog post 3",
    "section": "3. Data Visualization",
    "text": "3. Data Visualization"
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-locations-with-crime",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-locations-with-crime",
    "title": "Blog post 3",
    "section": "3.1 Top Locations with Crime",
    "text": "3.1 Top Locations with Crime\n\nggplot(data_clean, aes(x = reorder(AREA.NAME, AREA.NAME, function(x) -length(x)))) +\n  geom_bar(fill = \"steelblue\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Top Locations with Highest Crime\", x = \"Area\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis depicts the locations for the places with the highest crime count. We previously did this last week but I wasn’t sure what the graph produced was so I recreated it. Same data as last week but a better visualization. Personally, nothing to note here, it seems like crime is distributd pretty evenly throughout the ditricts. Can be something to be mentioned in the final project."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-frequency-over-time",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-frequency-over-time",
    "title": "Blog post 3",
    "section": "3.2 Crime Frequency Over Time",
    "text": "3.2 Crime Frequency Over Time\n\nggplot(data_clean, aes(x = month)) +\n  geom_bar(fill = \"darkred\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Crime Frequency Over Time\", x = \"Month\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis is a bit hard to read but I did find it interesting that there was a spike in crime in 2024 January-February. May look into further for potential findings on why that was. Otherwise, crime seems decently evenly distributed by time."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-counts-by-descent-description",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-counts-by-descent-description",
    "title": "Blog post 3",
    "section": "3.3 Crime Counts by Descent Description",
    "text": "3.3 Crime Counts by Descent Description\n\nggplot(data_clean, aes(x = reorder(Descent_Description, Descent_Description, function(x) -length(x)))) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Crime Counts by Descent Description\", x = \"Descent Category\", y = \"Count\")\n\n\n\n\n\n\n\n\nInteresting that Hispanics represent the largest amount of crime counts by description. Even more interesting, is that Whites are second given different American stereotypes. May look into doing a correlation chart on this."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-20-most-frequent-crime-types",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-20-most-frequent-crime-types",
    "title": "Blog post 3",
    "section": "3.4 Top 20 Most Frequent Crime Types",
    "text": "3.4 Top 20 Most Frequent Crime Types\n\n# Calculate the frequency of each crime type\ntop_crime_types &lt;- data_clean %&gt;%\n  count(Crm.Cd.Desc, sort = TRUE) %&gt;%\n  top_n(20, n)\n\nggplot(top_crime_types, aes(x = reorder(Crm.Cd.Desc, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Top 20 Most Frequent Crime Types\", x = \"Crime Type\", y = \"Count\")\n\n\n\n\n\n\n\n\nInitially looked at all types of crime but this was too large a list so it was condensed down to the above chart with the top 20 most frequent. May look into condensing some of these categories such as burglary and robbery and look into the difference between those two crimes to see if they can be condensed. I really want to treat the $950 above and below as the same category. Still, it is interesting to see that a lot of people are still willing to commit felonies after the revision of the burglary codes. The difference between the $950 over and under is not too much."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#monthly-crime-frequency-by-descent",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#monthly-crime-frequency-by-descent",
    "title": "Blog post 3",
    "section": "3.5 Monthly Crime Frequency by Descent",
    "text": "3.5 Monthly Crime Frequency by Descent\n\nggplot(data_clean, aes(x = month, fill = Descent_Description)) +\n  geom_bar(position = \"stack\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Monthly Crime Frequency by Descent\", x = \"Month\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis is a confusing chart but an interesting visualization. Could be used if it considered most frequent to least frequent but we will see if we can incorporate this somehow."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#next-steps",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#next-steps",
    "title": "Blog post 3",
    "section": "4. Next Steps",
    "text": "4. Next Steps\nMay look into saving the cleaned data as a seperate rds file depending on groupmate feedback. We will see. I will sleep on it.\nNeed to look at more charts like the places description to see if there are any indicators there. Also need to take a look at the time description to see which hours people are committing crime. I’m not doing anymore though at least not this week.\nLastly, look into formulating an idea for the direction of the project and any potential uses of the “lm()” function.\n##5. Charts about places and time that committing crime\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf&lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndf&lt;- readRDS(\"filtered_data_copy.rds\")\n\ndf &lt;- df %&gt;% select(-Mocodes) \n\nWe take out Mocodes since it’s a kind of code that use in police department. It’s hard for us to analyze some useful Information.\n\ndf &lt;- df %&gt;%\n  mutate(\n    TIME.OCC = sprintf(\"%04d\", TIME.OCC),  \n    TIME.OCC = format(strptime(TIME.OCC, format = \"%H%M\"), \"%H:%M\")  \n  )\n\nWe transform the type of TIME.OCC to become HH:MM instead of numerical. It’s easier for us to read because some of the time like 00:01 will be handle as 1 in the previous TIME.OCC column. We plus in the time in graph to see when would have the most case occurs.\n\nggplot(df, aes(x = TIME.OCC)) +\n  geom_bar(stat = \"count\", fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Count of TIME.OCC\", x = \"Time of Occurrence\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can’t see a lot of pattern because the graph is not sorted as hourly. We will take out HH from HH:MM and find the count of case to see in what hour in a day would have most case occurs.\n\ndf &lt;- df %&gt;%\n  mutate(Hour = substr(TIME.OCC, 1, 2))\nhead(df)\n\n# A tibble: 6 × 29\n   DR_NO Date.Rptd DATE.OCC TIME.OCC  AREA AREA.NAME Rpt.Dist.No Part.1.2 Crm.Cd\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;  &lt;int&gt;\n1 2.01e8 01/03/20… 01/03/2… 04:00        6 Hollywood         648        1    310\n2 2.00e8 12/05/20… 12/05/2… 01:00        2 Rampart           235        1    510\n3 2.10e8 02/02/20… 10/02/2… 15:50        1 Central           152        2    649\n4 2.01e8 07/29/20… 07/29/2… 10:35       13 Newton           1383        2    740\n5 2.02e8 06/28/20… 06/25/2… 14:55       16 Foothill         1695        2    354\n6 2.01e8 03/12/20… 03/10/2… 12:20        9 Van Nuys          994        1    310\n# ℹ 20 more variables: Crm.Cd.Desc &lt;chr&gt;, Vict.Age &lt;int&gt;, Vict.Sex &lt;chr&gt;,\n#   Vict.Descent &lt;chr&gt;, Premis.Cd &lt;int&gt;, Premis.Desc &lt;chr&gt;,\n#   Weapon.Used.Cd &lt;int&gt;, Weapon.Desc &lt;chr&gt;, Status &lt;chr&gt;, Status.Desc &lt;chr&gt;,\n#   Crm.Cd.1 &lt;int&gt;, Crm.Cd.2 &lt;int&gt;, Crm.Cd.3 &lt;int&gt;, Crm.Cd.4 &lt;int&gt;,\n#   LOCATION &lt;chr&gt;, Cross.Street &lt;chr&gt;, LAT &lt;dbl&gt;, LON &lt;dbl&gt;, year &lt;dbl&gt;,\n#   Hour &lt;chr&gt;\n\nhourly_counts &lt;- df %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(Count = n())\nggplot(hourly_counts, aes(x = Hour, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Occurrences by Hour\", x = \"Hour of the Day\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe answer is actually suprising because people thinks that mid night would be the time that will have most case. But it seems that noon time is would be the one that have the most case. It may be the reason that people may not realize and call the police after they wake up in the morning.\nThis is the most updated version."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#analyze-the-relationship-between-top-crimes-and-specific-areas.",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#analyze-the-relationship-between-top-crimes-and-specific-areas.",
    "title": "Blog post 3",
    "section": "6. Analyze the relationship between top crimes and specific areas.",
    "text": "6. Analyze the relationship between top crimes and specific areas.\n##6.1 analyze the highest crime in each of the top 10 areas.\n\n# Calculate total crime count by area and select top 10 areas\n\ndata &lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndata &lt;- readRDS(\"filtered_data_copy.rds\")\n\ncrime_area_summary &lt;- data %&gt;%\n  group_by(AREA.NAME, Crm.Cd.Desc) %&gt;%\n  summarise(Crime_Count = n())\n\n`summarise()` has grouped output by 'AREA.NAME'. You can override using the\n`.groups` argument.\n\ntop_10_areas &lt;- crime_area_summary %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarise(Total_Crimes = sum(Crime_Count)) %&gt;%\n  arrange(desc(Total_Crimes)) %&gt;%\n  top_n(10, Total_Crimes) %&gt;%\n  pull(AREA.NAME)  \n\n\n# Filter for top 10 areas\ntop_10_area_crimes &lt;- crime_area_summary %&gt;%\n  filter(AREA.NAME %in% top_10_areas)\n\n# Find the most common crime type in each top area\nhighest_crime_per_area &lt;- top_10_area_crimes %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  filter(Crime_Count == max(Crime_Count)) %&gt;%\n  select(AREA.NAME, Crm.Cd.Desc, Crime_Count) %&gt;%\n  arrange(desc(Crime_Count))\n\n\n# plot showing the highest crime in each of the top 10 areas\nggplot(highest_crime_per_area, aes(x = reorder(AREA.NAME, -Crime_Count), y = Crime_Count, fill = Crm.Cd.Desc)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Common Crime Type in Each of the Top 10 Areas\",\n       x = \"Area\",\n       y = \"Crime Count\",\n       fill = \"Crime Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis helps identifying which crime types are dominant in high-crime areas, which could be usedful for targeted policy-making. it is worthy noticing that vehicle stolen us a widespread issue across several high crime areas. burglary is the top crime in wilshire, showing a unique pattern of property crimes in that area compared to the others.\n##6.2 analyze which area is the most prevalent for each of the top 10 crimes\n\n# Identify the top 10 most frequent crime types\ntop_10_crimes &lt;- data_clean %&gt;%\n  count(Crm.Cd.Desc, sort = TRUE) %&gt;%\n  top_n(10, n) %&gt;%\n  pull(Crm.Cd.Desc) \n\n\n# Filter for the top 10 crimes\ntop_10_crime_areas &lt;- crime_area_summary %&gt;%\n  filter(Crm.Cd.Desc %in% top_10_crimes)\n\n# Find the most prevalent area for each top crime\nmost_prevalent_area_per_crime &lt;- top_10_crime_areas %&gt;%\n  group_by(Crm.Cd.Desc) %&gt;%\n  filter(Crime_Count == max(Crime_Count)) %&gt;%\n  select(Crm.Cd.Desc, AREA.NAME, Crime_Count) %&gt;%\n  arrange(desc(Crime_Count))\n\n\n# plot showing the most prevalent area for each of the top 10 crimes\nggplot(most_prevalent_area_per_crime, aes(x = reorder(Crm.Cd.Desc, -Crime_Count), y = Crime_Count, fill = AREA.NAME)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Prevalent Area for Each of the Top 10 Crimes\",\n       x = \"Crime Type\",\n       y = \"Crime Count\",\n       fill = \"Area\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nvehicle stolen and burglary from vehicle are particulary prevalent in 77th street and central. assault with deadly weapon, aggravated assault and battery simple assault also occur very frequently in 77th street. theft of identity is most prevalent in N Hollywood, which shows a different crime pattern in this area\ncombining with these two charts together, vehicle theft and burglary from vehicles dominate as the most frequent crime types in multiple areas, especially in 77th street and central. the central area has a high incidence of various crimes, which may indicating that the central area faces broader challenges.\n\n# Convert the date column to Date format if not already in that format\ndata$DATE.OCC &lt;- as.POSIXct(data$DATE.OCC, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n# Create an age group column\ndata &lt;- data %&gt;%\n  mutate(Age_Group = case_when(\n    Vict.Age &lt; 18 ~ \"Under 18\",\n    Vict.Age &gt;= 18 & Vict.Age &lt; 30 ~ \"18-29\",\n    Vict.Age &gt;= 30 & Vict.Age &lt; 45 ~ \"30-44\",\n    Vict.Age &gt;= 45 & Vict.Age &lt; 60 ~ \"45-59\",\n    Vict.Age &gt;= 60 ~ \"60+\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# Replace missing or \"X\" gender values with \"Unknown\"\ndata &lt;- data %&gt;%\n  mutate(Vict.Sex = ifelse(is.na(Vict.Sex) | Vict.Sex == \"X\", \"Unknown\", Vict.Sex))\n\n# Plot the distribution of crimes by age group and gender, including unknown gender\nggplot(data, aes(x = Age_Group, fill = Vict.Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Distribution of Crimes by Victim's Age Group and Gender\",\n       x = \"Age Group\",\n       y = \"Number of Crimes\",\n       fill = \"Gender\") +\n  scale_fill_manual(values = c(\"F\" = \"green\", \"M\" = \"purple\", \"H\" = \"cyan\", \"Unknown\" = \"orange\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe highest number of crimes appears to target females under 18, represented by the red bar in the “Under 18” category. This could indicate that crimes against younger females are a significant issue. Additionally The bar heights suggest that crimes involving male victims increase steadily from younger to older age groups, peaking around 30-44 and 45-59 age brackets. After 60, crime incidence significantly declines for all genders and Younger individuals (under 18 and 18-29) seem to experience a higher number of crimes overall compared to older individuals, indicating that crime incidents decrease with age."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-10-07-blog-post-1/blog-post-1.html",
    "href": "posts/2024-10-07-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "##Dataset 1## https://www.kaggle.com/datasets/iamsouravbanerjee/airline-dataset Airline dataset\nThis dataset has about 15 columns with mainly incorporating fields such as Passenger ID, First Name, Last Name, Gender, Age, Nationality, Airport Name, Airport Country Code, Country Name, Airport Continent, Continents, Departure Date, Arrival Airport, Pilot Name, and Flight Status. As for the amount of rows, there are 98,619 unique ID names indicating at least 98,619 rows. The dataset was originally collected due to the information that the aviation industry can give. The dataset says that, “It provides valuable information about flight routes, schedules, passenger demographics, and preferences, which airlines can leverage to optimize their operations and enhance customer experiences.” It can also be important for governing bodies for security purposes in terms of the people entering the country. As for how they collected this data, they collected it probably through ticketing.\nThe dataset doesn’t contain any missing values, but improvements can be made to the qualitative and categorical data by standardizing certain fields. For example, we can convert categorical variables like “Gender” and “Flight Status” into numerical values for easier analysis. Potential research questions for this dataset could be: What is the trend/distribution of nationalities among passengers flying from specific airports, what factors are associated with flight delays compared to different variables, and whether pilot performance can be related to flight delay. One challenge we may face for this dataset is if multiple pilots are assigned to the same flight, it may be difficult to analyze how the pilot’s behavior affected flight performance.\n##Dataset 2## https://wonder.cdc.gov/controller/datarequest/D198;jsessionid=2282EA995E0480CCB5AF9F14471B\n75,000 rows and 6 columns. “Cancer Sites” “Region”“State”“Year”“Race” Count Data was collected by the CDC. Unknown how they were collected.\nCollected because it is important to collect cancer information. I can load it as a .txt but unknown if I can get it as a readable csv or something of that sort. May need to be cleaned.\nKey questions for analysis could include: how cancer incidence rates vary between racial groups within the same state and year, changes in cancer cases over time, and regional comparisons. Some challenges in analyzing this data might include handling missing or incomplete data if it exists, accurately interpreting the racial and region codes, and ensuring that year-to-year comparisons account for other variables that might affect cancer rates.\n##Dataset 3## https://usa.ipums.org/usa-action/variables/group 3373378 rows and 14 columns Columns: YEAR, HHINCOME, RACE, RACED, RACAMIND, RACASIAN, RACBLK, RACPACIS RACWHT, RACOTHER, EDUC, EDUCD, SCHLTYPE, INCWAGE\nThe data from IPUMS USA is primarily collected through U.S. census records and surveys. The data is consistently coded across census years, allowing for longitudinal studies on demographic, geographic, and economic trends.\nThe sample we select is from American Community Survey 2022, here are the key characteristics of this sample: a) 1-in-100 national random sample of the population. b) The data include persons in group quarters. c) This is a weighted sample. d) The smallest identifiable geographic unit is the PUMA, containing at least 100,000 persons. PUMAs do not cross state boundaries.\nWe are allowed to load and clean the data, the dataset doesn’t contain any missing values (is.na=0).\nThe main questions based on the variables we selected at this time can be: what’s the relationship between educational attainment and income wages across different racial groups, how does educational levels affect the income for a person across different racial groups, and how have trends in household income varied across different racial groups over time, considering differences in educational attainment, wage income, and school type. One challenge we may face is data cleaning. For example, the variable HHINCOME is a 7-digit numeric code that reports the total money income of all household members age 15+ during the previous year. However, the value 9999999 in the dataset represents missing values (N/A) for household income. There are numerous rows in the dataset with this value, and filtering out these rows could be time-consuming and would require careful attention to ensure the accuracy of the analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#zihao-guo",
    "href": "about.html#zihao-guo",
    "title": "About",
    "section": "Zihao Guo",
    "text": "Zihao Guo\nZihao is a Junior in Data Science and Statistics."
  },
  {
    "objectID": "about.html#sean-fung",
    "href": "about.html#sean-fung",
    "title": "About",
    "section": "Sean Fung",
    "text": "Sean Fung\nSean is a senior in Math and Computer Science"
  },
  {
    "objectID": "about.html#oscar-mo",
    "href": "about.html#oscar-mo",
    "title": "About",
    "section": "Oscar Mo",
    "text": "Oscar Mo\nOscar is a senior in Computer Science"
  },
  {
    "objectID": "about.html#siqi-chen",
    "href": "about.html#siqi-chen",
    "title": "About",
    "section": "Siqi Chen",
    "text": "Siqi Chen\nSiqi is a junior in Applied Mathematics"
  },
  {
    "objectID": "about.html#yawen-zhang",
    "href": "about.html#yawen-zhang",
    "title": "About",
    "section": "Yawen Zhang",
    "text": "Yawen Zhang\nYawen is a senior in Applied Mathematics\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]