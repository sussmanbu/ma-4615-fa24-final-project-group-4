[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n&gt; library(purrr)\n\n&gt; library(tidyr)\n\n&gt; library(here)\n\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA415 G4 Final Project\n\n\n\n&gt; data &lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\n&gt; data$DATE.OCC &lt;- as.POSIXct(data$DATE.OCC, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n&gt; data$year &lt;- as.numeric(format(data$DATE.OCC, \"%Y\"))\n\n&gt; filtered_years &lt;- data %&gt;% filter(year %in% 2020:2024)\n\n&gt; descent_codes &lt;- tribble(~Descent_Code, ~Descent_Description, \n+     \"A\", \"Other Asian\", \"B\", \"Black\", \"C\", \"Chinese\", \"D\", \"Cambodian\", \n+     \"F\", .... [TRUNCATED] \n\n&gt; data_clean &lt;- filtered_years %&gt;% left_join(descent_codes, \n+     by = c(Vict.Descent = \"Descent_Code\"))\n\n&gt; data_clean$Descent_Description[is.na(data_clean$Descent_Description)] &lt;- \"Unknown\"\n\n&gt; data_clean &lt;- data_clean %&gt;% select(-c(Crm.Cd.2, Crm.Cd.3, \n+     Crm.Cd.4, Weapon.Used.Cd, Weapon.Desc, Cross.Street))\n\n&gt; data_clean$month &lt;- format(data_clean$DATE.OCC, \"%Y-%m\")\n\n&gt; sampled_data &lt;- data_clean %&gt;% group_split(year) %&gt;% \n+     map_df(~slice_sample(.x, n = min(2500, nrow(.x))))\n\n&gt; saveRDS(sampled_data, \"filtered_data_2.rds\")\n\n&gt; nrow(sampled_data)\n[1] 12500\n\n&gt; str(sampled_data)\ntibble [12,500 × 25] (S3: tbl_df/tbl/data.frame)\n $ DR_NO              : int [1:12500] 202108005 200808511 201510886 200119913 201418152 200314438 202110690 200400719 201512525 201904784 ...\n $ Date.Rptd          : chr [1:12500] \"04/04/2020 12:00:00 AM\" \"04/20/2020 12:00:00 AM\" \"06/04/2020 12:00:00 AM\" \"10/24/2020 12:00:00 AM\" ...\n $ DATE.OCC           : POSIXct[1:12500], format: \"2020-04-01\" \"2020-04-04\" ...\n $ TIME.OCC           : int [1:12500] 1300 1500 1400 1300 2030 440 100 1500 2035 120 ...\n $ AREA               : int [1:12500] 21 8 15 1 14 3 21 4 15 19 ...\n $ AREA.NAME          : chr [1:12500] \"Topanga\" \"West LA\" \"N Hollywood\" \"Central\" ...\n $ Rpt.Dist.No        : int [1:12500] 2155 834 1557 158 1456 376 2141 479 1526 1902 ...\n $ Part.1.2           : int [1:12500] 2 2 1 1 1 1 2 1 1 1 ...\n $ Crm.Cd             : int [1:12500] 740 649 510 310 230 210 626 230 230 230 ...\n $ Crm.Cd.Desc        : chr [1:12500] \"VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)\" \"DOCUMENT FORGERY / STOLEN FELONY\" \"VEHICLE - STOLEN\" \"BURGLARY\" ...\n $ Mocodes            : chr [1:12500] \"1307 0329\" \"1202 0100 0923 1822\" \"\" \"1822 1202 0344\" ...\n $ Vict.Age           : int [1:12500] 35 62 0 60 55 36 28 19 81 37 ...\n $ Vict.Sex           : chr [1:12500] \"M\" \"M\" \"\" \"M\" ...\n $ Vict.Descent       : chr [1:12500] \"W\" \"W\" \"\" \"W\" ...\n $ Premis.Cd          : int [1:12500] 101 502 108 502 102 502 501 504 501 101 ...\n $ Premis.Desc        : chr [1:12500] \"STREET\" \"MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC)\" \"PARKING LOT\" \"MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC)\" ...\n $ Status             : chr [1:12500] \"IC\" \"IC\" \"IC\" \"IC\" ...\n $ Status.Desc        : chr [1:12500] \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" ...\n $ Crm.Cd.1           : int [1:12500] 740 649 510 310 230 210 626 230 230 230 ...\n $ LOCATION           : chr [1:12500] \"CALVERT\" \"10500    ASHTON                       AV\" \"10700    BLIX                         ST\" \"800 E  6TH                          ST\" ...\n $ LAT                : num [1:12500] 34.2 34.1 34.2 34 34 ...\n $ LON                : num [1:12500] -119 -118 -118 -118 -118 ...\n $ year               : num [1:12500] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n $ Descent_Description: chr [1:12500] \"White\" \"White\" \"Unknown\" \"White\" ...\n $ month              : chr [1:12500] \"2020-04\" \"2020-04\" \"2020-05\" \"2020-07\" ...\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "data.html#data-overview",
    "href": "data.html#data-overview",
    "title": "Data",
    "section": "1. Data Overview",
    "text": "1. Data Overview\nThe dataset contains reported crimes in Los Angeles from 2020 onward, sourced from the LAPD.\n\nSource: LAPD Crime Data (2020–Present)\nAttribution: Derived from original crime reports typed on paper.\nPurpose:\n\nTrack crime patterns.\nAllocate public safety resources.\nPromote transparency and public accountability.\n\nChallenges:\n\nLarge dataset size (~987K rows, 28 variables).\nCleaning required to address duplicates, incomplete data, and usability for spatial/temporal analysis."
  },
  {
    "objectID": "data.html#variables-in-the-dataset",
    "href": "data.html#variables-in-the-dataset",
    "title": "Data",
    "section": "2. Variables in the Dataset",
    "text": "2. Variables in the Dataset\nThe dataset comprises 28 columns, described below:\n\n\n\n\n\n\n\n\nVariable\nDescription\nData Type\n\n\n\n\nDR_NO\nDivision of Records Number: Unique file identifier (YY+Area ID+5 digits).\nText\n\n\nDate Rptd\nDate crime was reported (MM/DD/YYYY).\nTimestamp\n\n\nDATE OCC\nDate crime occurred (MM/DD/YYYY).\nTimestamp\n\n\nTIME OCC\nTime crime occurred (24-hour format).\nText\n\n\nAREA\nLAPD’s 21 geographic areas (1–21).\nText\n\n\nAREA NAME\nName corresponding to the geographic area (e.g., 77th Street).\nText\n\n\nRpt Dist No\nSub-area code within a geographic area.\nText\n\n\nPart 1-2\nClassification of crime as Part I or Part II.\nNumber\n\n\nCrm Cd\nPrimary crime code.\nText\n\n\nCrm Cd Desc\nDescription of the primary crime.\nText\n\n\nMocodes\nSuspect activities during the crime.\nText\n\n\nVict Age\nAge of the victim.\nNumber\n\n\nVict Sex\nVictim’s gender (M: Male, F: Female, X: Unknown).\nText\n\n\nVict Descent\nEthnic descent (e.g., H: Hispanic, W: White).\nText\n\n\nPremis Cd\nCode for the type of location where the crime occurred.\nNumber\n\n\nPremis Desc\nDescription of the premise code.\nText\n\n\nWeapon Used Cd\nCode for the weapon used.\nText\n\n\nWeapon Desc\nDescription of the weapon code.\nText\n\n\nStatus\nStatus of the case (e.g., IC: Incomplete).\nText\n\n\nStatus Desc\nDescription of the status code.\nText\n\n\nCrm Cd 1–4\nPrimary and additional crime codes.\nText\n\n\nLOCATION\nApproximate address of the crime (anonymized).\nText\n\n\nCross Street\nCross street near the crime location.\nText\n\n\nLAT\nLatitude of the crime location.\nNumber\n\n\nLON\nLongitude of the crime location.\nNumber"
  },
  {
    "objectID": "data.html#cleaning-and-sampling",
    "href": "data.html#cleaning-and-sampling",
    "title": "Data",
    "section": "3. Cleaning and Sampling",
    "text": "3. Cleaning and Sampling\n\nInitial Dataset Loading\nThe dataset was loaded as a CSV file for easier manipulation. Here’s the R code for loading it:\n\nlibrary(here)\nlibrary(readr)\ndata &lt;- read_csv(here(\"your_file.csv\"))\n\nThen we start the data cleaning process.\nTo reduce the dataset size (because it was too big previously) and balance representation across years, we:\n\n\nExtracted the year from the DATE OCC column.\n\nFiltered data for years 2020–2024.\n\nSampled up to 2,500 rows per year for efficiency.\n\n\nThe following r chunk is the exact code we use by following the previous steps.\n\n# Convert the date column to year format\ndata$year &lt;- format(as.Date(data$DATE.OCC, format = \"%m/%d/%Y\"), \"%Y\")\n# Convert year to numeric\ndata$year &lt;- as.numeric(data$year)  \n# Filter for the years 2020–2024\nfiltered_years &lt;- data %&gt;%\n  filter(year %in% 2020:2024)\n# Split by year and sample up to 2,500 rows per year\nsampled_data &lt;- filtered_years %&gt;%\n  group_split(year) %&gt;%\n  map_df(~ slice_sample(.x, n = min(2500, nrow(.x))))\n\nThe RDS file after first time cleaning can look like this (some columns filtered out for readability):"
  },
  {
    "objectID": "data.html#updated-dataset-with-years-spread-across-multiple-years",
    "href": "data.html#updated-dataset-with-years-spread-across-multiple-years",
    "title": "Data",
    "section": "Updated Dataset with Years Spread Across Multiple Years",
    "text": "Updated Dataset with Years Spread Across Multiple Years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDR_NO\nDATE.OCC\nYear\nAREA.NAME\nCrm.Cd\nCrm.Cd.Desc\nPremis.Desc\nLAT\nLON\n\n\n\n\n200604124\n01/03/2020\n2020\nHollywood\n310\nBURGLARY\nRESTAURANT/FAST FOOD\n34.1000\n-118.3113\n\n\n200218223\n12/05/2019\n2019\nRampart\n510\nVEHICLE - STOLEN\nSTREET\n34.0707\n-118.2693\n\n\n210105387\n02/02/2021\n2021\nCentral\n649\nDOCUMENT FORGERY / STOLEN FELONY\nBANK\n34.0493\n-118.2582\n\n\n201314683\n07/29/2020\n2020\nNewton\n740\nVANDALISM - FELONY\nVEHICLE, PASSENGER/TRUCK\n33.9830\n-118.2783\n\n\n201610483\n06/28/2022\n2022\nFoothill\n354\nTHEFT OF IDENTITY\nSINGLE FAMILY DWELLING\n34.2103\n-118.3747\n\n\n200905047\n01/25/2019\n2019\nVan Nuys\n745\nVANDALISM - MISDEMEANOR ($399 OR UNDER)\nVEHICLE STORAGE LOT\n34.1961\n-118.4487\n\n\n200111368\n05/05/2021\n2021\nCentral\n940\nEXTORTION\nMULTI-UNIT DWELLING\n34.0433\n-118.2377\n\n\n201508876\n04/16/2022\n2022\nN Hollywood\n310\nBURGLARY\nLAUNDROMAT\n34.1649\n-118.3615"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA415 G4 Final Project\n\nLoading required package: viridisLite\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\nGetting data from the 2016-2020 5-year ACS\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries"
  },
  {
    "objectID": "big_picture.html#beyond-the-poverty-myth-unraveling-los-angeles-crime-landscape",
    "href": "big_picture.html#beyond-the-poverty-myth-unraveling-los-angeles-crime-landscape",
    "title": "Big Picture",
    "section": "Beyond the Poverty Myth: Unraveling Los Angeles’ Crime Landscape",
    "text": "Beyond the Poverty Myth: Unraveling Los Angeles’ Crime Landscape\nWhen you think about crime, you might picture dangerous streets in low-income neighborhoods and pristine, secure communities in wealthier areas. Normally, you would avoid that one sketchy neighborhood when it is late at night. It’s a common narrative, repeated so often that it feels like fact: crime is a symptom of poverty, and wealth provides a safety net against it. But is this common perception really the truth?\nUsing the LAPD crime reports and Census data from the past 4 years, we set out to explore the connection between income levels and crime rates across different neighborhoods in Los Angeles. The results might surprise you. While crime does concentrate in economically disadvantaged areas, the relationship between median income and crime is far from straightforward—and some of our findings challenge these well-worn assumptions. The stereotype that low median areas experience higher crime rates is overly simplistic. While violent crimes show some correlation with lower-income neighborhoods, broader crime patterns reveal that income alone does not explain crime rates. Our analysis uncovers complexities and outliers that challenge conventional beliefs.\nThe idea that crime is concentrated solely in low-income areas is pervasive but overly simplistic. While economically disadvantaged neighborhoods often face greater crime challenges, wealthier areas aren’t immune.\n\n\n\n\n\n\n\n\n\nThis heatmap visualizes the total crime distribution across Los Angeles neighborhoods under LAPD jurisdiction. Darker areas indicate lower crime counts, while lighter areas signify higher crime activity. At a glance, the map reveals significant disparities in crime concentration across the city. High counts of crime are apparent in neighborhoods in Central and West.\n\n\n\n\n\n\n\n\n\nTo further examine the relationship between income and crime, we analyzed crime rates across neighborhoods, sorting them by median income. Based on common conception, most people would expect to see a decreasing trend where neighborhoods with lower median income would have higher crime count. However, notable neighborhoods like Pacific and Central have the higher crime counts among Los Angeles neighborhoods that sit in the upper echelon of median income.\n\n\n\n\n\n\n\n\n\nThis scatterplot shows the relationship between median income and total crimes across neighborhoods in Los Angeles. Each point represents a neighborhood, with the x-axis showing median income and the y-axis showing the total number of crimes. The colors range from dark purple to bright yellow, indicating neighborhoods with lower to higher median incomes, respectively. At first look, the graph reveals no clear or consistent correlation between median income and total crime levels. While some low-income neighborhoods (darker points) experience higher crime rates, several high-income neighborhoods (yellow points) also show significant crime activity. This pattern underscores that the relationship between income and crime is not straightforward. Factors beyond income, such as population density, local policies, or social dynamics, likely play a significant role in shaping crime rates. This finding challenges traditional stereotypes and invites a deeper examination of what drives crime in Los Angeles communities.\nCrime in Los Angeles is a multifaceted issue. Our analysis challenges the simplistic narrative linking poverty and crime, showing that even wealthy neighborhoods can face significant crime challenges. These findings underscore the need to look beyond income when addressing public safety and building stronger communities.\n\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should also include a small interactive dashboard. The dashboard should be created either using Shinylive, as below. This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template.\n\n\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should also include a small interactive dashboard. The dashboard should be created either using Shinylive, as below. This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page-1",
    "href": "big_picture.html#rubric-on-this-page-1",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components-1",
    "href": "big_picture.html#rubric-other-components-1",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#zihao-guo",
    "href": "about.html#zihao-guo",
    "title": "About",
    "section": "Zihao Guo",
    "text": "Zihao Guo\nZihao is a Junior in Data Science and Statistics."
  },
  {
    "objectID": "about.html#sean-fung",
    "href": "about.html#sean-fung",
    "title": "About",
    "section": "Sean Fung",
    "text": "Sean Fung\nSean is a senior in Math and Computer Science"
  },
  {
    "objectID": "about.html#oscar-mo",
    "href": "about.html#oscar-mo",
    "title": "About",
    "section": "Oscar Mo",
    "text": "Oscar Mo\nOscar is a senior in Computer Science"
  },
  {
    "objectID": "about.html#siqi-chen",
    "href": "about.html#siqi-chen",
    "title": "About",
    "section": "Siqi Chen",
    "text": "Siqi Chen\nSiqi is a junior in Applied Mathematics"
  },
  {
    "objectID": "about.html#yawen-zhang",
    "href": "about.html#yawen-zhang",
    "title": "About",
    "section": "Yawen Zhang",
    "text": "Yawen Zhang\nYawen is a senior in Applied Mathematics\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "posts/2024-10-07-blog-post-1/blog-post-1.html",
    "href": "posts/2024-10-07-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "##Dataset 1## https://www.kaggle.com/datasets/iamsouravbanerjee/airline-dataset Airline dataset\nThis dataset has about 15 columns with mainly incorporating fields such as Passenger ID, First Name, Last Name, Gender, Age, Nationality, Airport Name, Airport Country Code, Country Name, Airport Continent, Continents, Departure Date, Arrival Airport, Pilot Name, and Flight Status. As for the amount of rows, there are 98,619 unique ID names indicating at least 98,619 rows. The dataset was originally collected due to the information that the aviation industry can give. The dataset says that, “It provides valuable information about flight routes, schedules, passenger demographics, and preferences, which airlines can leverage to optimize their operations and enhance customer experiences.” It can also be important for governing bodies for security purposes in terms of the people entering the country. As for how they collected this data, they collected it probably through ticketing.\nThe dataset doesn’t contain any missing values, but improvements can be made to the qualitative and categorical data by standardizing certain fields. For example, we can convert categorical variables like “Gender” and “Flight Status” into numerical values for easier analysis. Potential research questions for this dataset could be: What is the trend/distribution of nationalities among passengers flying from specific airports, what factors are associated with flight delays compared to different variables, and whether pilot performance can be related to flight delay. One challenge we may face for this dataset is if multiple pilots are assigned to the same flight, it may be difficult to analyze how the pilot’s behavior affected flight performance.\n##Dataset 2## https://wonder.cdc.gov/controller/datarequest/D198;jsessionid=2282EA995E0480CCB5AF9F14471B\n75,000 rows and 6 columns. “Cancer Sites” “Region”“State”“Year”“Race” Count Data was collected by the CDC. Unknown how they were collected.\nCollected because it is important to collect cancer information. I can load it as a .txt but unknown if I can get it as a readable csv or something of that sort. May need to be cleaned.\nKey questions for analysis could include: how cancer incidence rates vary between racial groups within the same state and year, changes in cancer cases over time, and regional comparisons. Some challenges in analyzing this data might include handling missing or incomplete data if it exists, accurately interpreting the racial and region codes, and ensuring that year-to-year comparisons account for other variables that might affect cancer rates.\n##Dataset 3## https://usa.ipums.org/usa-action/variables/group 3373378 rows and 14 columns Columns: YEAR, HHINCOME, RACE, RACED, RACAMIND, RACASIAN, RACBLK, RACPACIS RACWHT, RACOTHER, EDUC, EDUCD, SCHLTYPE, INCWAGE\nThe data from IPUMS USA is primarily collected through U.S. census records and surveys. The data is consistently coded across census years, allowing for longitudinal studies on demographic, geographic, and economic trends.\nThe sample we select is from American Community Survey 2022, here are the key characteristics of this sample: a) 1-in-100 national random sample of the population. b) The data include persons in group quarters. c) This is a weighted sample. d) The smallest identifiable geographic unit is the PUMA, containing at least 100,000 persons. PUMAs do not cross state boundaries.\nWe are allowed to load and clean the data, the dataset doesn’t contain any missing values (is.na=0).\nThe main questions based on the variables we selected at this time can be: what’s the relationship between educational attainment and income wages across different racial groups, how does educational levels affect the income for a person across different racial groups, and how have trends in household income varied across different racial groups over time, considering differences in educational attainment, wage income, and school type. One challenge we may face is data cleaning. For example, the variable HHINCOME is a 7-digit numeric code that reports the total money income of all household members age 15+ during the previous year. However, the value 9999999 in the dataset represents missing values (N/A) for household income. There are numerous rows in the dataset with this value, and filtering out these rows could be time-consuming and would require careful attention to ensure the accuracy of the analysis."
  },
  {
    "objectID": "posts/2024-12-05-blog-post-7-/blog-post-7-.html",
    "href": "posts/2024-12-05-blog-post-7-/blog-post-7-.html",
    "title": "blog post 7",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nhere() starts at /Users/zzzhao/Desktop/MA415/ma-4615-fa24-final-project-group-4\n\nLoading required package: viridisLite\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\nGetting data from the 2016-2020 5-year ACS\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n# Update visualizations\nggplot(crime_summary_lapd, aes(x = avg_income, y = total_crimes)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Crime vs Median Income (LAPD Jurisdiction)\", x = \"Median Income\", y = \"Total Crimes\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  # Median income layer\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  labs(\n    title = \"Median Income within LAPD Jurisdiction\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Focus on the Los Angeles area\n    ylim = c(33, 35)       # Focus on the Los Angeles area\n  )\n\n\n\n\n\n\n\n\n\ncrime_within_lapd &lt;- st_transform(crime_within_lapd, st_crs(economic_data_lapd))\n\nggplot() +\n  # Median income heatmap\n  geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  \n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  # Overlay crime points\n  geom_sf(data = crime_within_lapd, aes(), color = \"black\", shape = 4, size = 0.5, alpha = 0.7) +\n  labs(\n    title = \"Median Income and Crime Data within LAPD Jurisdiction\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Focus on the Los Angeles area\n    ylim = c(33, 35)       # Focus on the Los Angeles area\n  )\n\n\n\n\n\n\n\n#Good place to use shinylive here. Can filter by year. \n\n# crime_filtered &lt;- crime_within_lapd %&gt;%\n#   filter(year == 2021)\n# ggplot() +\n#   geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  \n#   scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n#   geom_sf(data = crime_filtered, aes(), color = \"black\", shape = 4, size = 0.5, alpha = 0.7) +\n#   labs(\n#     title = \"Median Income and 2020 Crime Data within LAPD Jurisdiction\",\n#     fill = \"Median Income\"\n#   ) +\n#   theme_minimal() +\n#   coord_sf(xlim = c(-119, -117), ylim = c(33, 35))\n\n#Look at data utilizing a poisson distribution model. \n# Fit a Poisson regression model\n\n\ncrime_summary_race &lt;- crime_with_income %&gt;%\n  group_by(GEOID, Descent_Description) %&gt;%\n  summarize(\n    total_crimes = n(),  # Count crimes for each GEOID and Descent_Description group\n    avg_median_income = mean(estimate, na.rm = TRUE)\n  ) %&gt;%\n  pivot_wider(\n    names_from = Descent_Description,\n    values_from = total_crimes,\n    values_fill = 0  # Fill missing values with 0 for crimes\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    total_crimes = sum(c_across(-c(GEOID, avg_median_income, geometry)), na.rm = TRUE)  # Sum only the crime columns\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'GEOID'. You can override using the\n`.groups` argument.\n\n\n\ncrime_summary_race &lt;- crime_summary_race %&gt;%\n  rename_with(~ make.names(.), everything())\n\nbivar_poisson_model &lt;- glm(\n  total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + White + Black + Other + \n    Other.Asian + Unknown + Japanese + Korean + Filipino + Chinese + Hawaiian + \n    Asian.Indian + American.Indian.Alaskan.Native + Cambodian + Vietnamese + Laotian,\n  family = poisson(link = \"log\"),\n  data = crime_summary_race\n)\nsummary(bivar_poisson_model)\n\n\nCall:\nglm(formula = total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + \n    White + Black + Other + Other.Asian + Unknown + Japanese + \n    Korean + Filipino + Chinese + Hawaiian + Asian.Indian + American.Indian.Alaskan.Native + \n    Cambodian + Vietnamese + Laotian, family = poisson(link = \"log\"), \n    data = crime_summary_race)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     5.342e-01  2.044e-02  26.133   &lt;2e-16 ***\navg_median_income              -2.736e-06  2.547e-07 -10.742   &lt;2e-16 ***\nHispanic.Latin.Mexican          1.655e-01  2.937e-03  56.356   &lt;2e-16 ***\nWhite                           2.038e-01  5.124e-03  39.780   &lt;2e-16 ***\nBlack                           1.700e-01  4.936e-03  34.437   &lt;2e-16 ***\nOther                           1.790e-01  1.227e-02  14.593   &lt;2e-16 ***\nOther.Asian                     2.020e-02  4.523e-02   0.447   0.6552    \nUnknown                         1.032e-01  1.612e-03  64.060   &lt;2e-16 ***\nJapanese                       -3.522e-02  1.605e-01  -0.219   0.8263    \nKorean                          8.766e-02  7.250e-02   1.209   0.2266    \nFilipino                       -1.699e-01  1.118e-01  -1.519   0.1287    \nChinese                         1.525e-01  8.545e-02   1.784   0.0744 .  \nHawaiian                       -2.246e-01  3.782e-01  -0.594   0.5525    \nAsian.Indian                   -1.029e-01  3.336e-01  -0.309   0.7577    \nAmerican.Indian.Alaskan.Native -1.647e-01  1.950e-01  -0.845   0.3984    \nCambodian                      -2.265e-02  3.162e-01  -0.072   0.9429    \nVietnamese                     -2.529e-01  1.965e-01  -1.287   0.1980    \nLaotian                        -3.387e-01  1.000e+00  -0.339   0.7348    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6173.9  on 5564  degrees of freedom\nResidual deviance: 2074.0  on 5547  degrees of freedom\n  (319 observations deleted due to missingness)\nAIC: 15858\n\nNumber of Fisher Scoring iterations: 6\n\nbivar_nb_model &lt;- glm.nb(total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + White + Black + Other + \n    Other.Asian + Unknown + Japanese + Korean + Filipino + Chinese + Hawaiian + \n    Asian.Indian + American.Indian.Alaskan.Native + Cambodian + Vietnamese + Laotian, data = crime_summary_race)\n\nWarning in glm.nb(total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + :\nalternation limit reached\n\nsummary(bivar_nb_model)\n\n\nCall:\nglm.nb(formula = total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + \n    White + Black + Other + Other.Asian + Unknown + Japanese + \n    Korean + Filipino + Chinese + Hawaiian + Asian.Indian + American.Indian.Alaskan.Native + \n    Cambodian + Vietnamese + Laotian, data = crime_summary_race, \n    init.theta = 24.74819092, link = log)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     2.985e-01  2.278e-02  13.104  &lt; 2e-16 ***\navg_median_income              -1.531e-06  2.654e-07  -5.769 7.95e-09 ***\nHispanic.Latin.Mexican          2.251e-01  4.221e-03  53.326  &lt; 2e-16 ***\nWhite                           2.348e-01  6.021e-03  38.992  &lt; 2e-16 ***\nBlack                           2.165e-01  6.348e-03  34.107  &lt; 2e-16 ***\nOther                           2.142e-01  1.342e-02  15.957  &lt; 2e-16 ***\nOther.Asian                     1.222e-01  4.625e-02   2.641  0.00826 ** \nUnknown                         1.757e-01  2.853e-03  61.570  &lt; 2e-16 ***\nJapanese                        5.779e-02  1.665e-01   0.347  0.72856    \nKorean                          1.780e-01  7.480e-02   2.380  0.01733 *  \nFilipino                       -3.709e-02  1.146e-01  -0.324  0.74630    \nChinese                         1.953e-01  8.929e-02   2.187  0.02874 *  \nHawaiian                       -8.249e-02  3.873e-01  -0.213  0.83136    \nAsian.Indian                    9.135e-03  3.431e-01   0.027  0.97876    \nAmerican.Indian.Alaskan.Native -1.634e-02  1.994e-01  -0.082  0.93470    \nCambodian                       9.126e-02  3.242e-01   0.282  0.77831    \nVietnamese                     -1.173e-01  2.009e-01  -0.584  0.55944    \nLaotian                        -1.891e-01  1.020e+00  -0.185  0.85294    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(24.7489) family taken to be 1)\n\n    Null deviance: 5477.7  on 5564  degrees of freedom\nResidual deviance: 1349.0  on 5547  degrees of freedom\n  (319 observations deleted due to missingness)\nAIC: 15580\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  24.75 \n          Std. Err.:  1.49 \nWarning while fitting theta: alternation limit reached \n\n 2 x log-likelihood:  -15541.83 \n\n\n\ncrime_summary_race_model &lt;- crime_summary_race %&gt;%\n  filter(complete.cases(\n    avg_median_income, Hispanic.Latin.Mexican, White, Black, Other, Other.Asian,\n    Unknown, Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian,\n    American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian\n  ))\n\n# Add predicted values to the filtered data\ncrime_summary_race_model &lt;- crime_summary_race_model %&gt;%\n  mutate(predicted_crimes = predict(bivar_nb_model, type = \"response\"))\n\ncrime_race_long &lt;- crime_summary_race_model %&gt;%\n  pivot_longer(\n    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, \n             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, \n             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),\n    names_to = \"race\",\n    values_to = \"crime_count\"\n  )\n\n\ncrime_race_long &lt;- crime_summary_race %&gt;%\n  pivot_longer(\n    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, \n             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, \n             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),\n    names_to = \"race\",\n    values_to = \"crime_count\"\n  ) %&gt;%\n  filter(!is.na(crime_count) & crime_count &gt; 0) %&gt;%\n  mutate(avg_median_income = avg_median_income / 1000)\n\n\n\n# Fit Poisson regression with pivoted data\npoisson_model &lt;- glm(\n  crime_count ~ avg_median_income + race,\n  family = poisson(link = \"log\"),\n  data = crime_race_long\n)\nsummary(poisson_model)\n\n\nCall:\nglm(formula = crime_count ~ avg_median_income + race, family = poisson(link = \"log\"), \n    data = crime_race_long)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 0.1632419  0.2298224   0.710 0.477521    \navg_median_income          -0.0019821  0.0002549  -7.777 7.42e-15 ***\nraceAsian.Indian            0.0155105  0.4422303   0.035 0.972021    \nraceBlack                   0.6206238  0.2308355   2.689 0.007175 ** \nraceCambodian               0.1967529  0.5026375   0.391 0.695471    \nraceChinese                 0.1221015  0.2668446   0.458 0.647257    \nraceFilipino               -0.0125247  0.2612828  -0.048 0.961768    \nraceHawaiian               -0.0256375  0.4683073  -0.055 0.956342    \nraceHispanic.Latin.Mexican  0.8102344  0.2300788   3.522 0.000429 ***\nraceJapanese                0.0635533  0.3100358   0.205 0.837582    \nraceKorean                  0.1226668  0.2558546   0.479 0.631626    \nraceLaotian                -0.0216528  1.0259884  -0.021 0.983162    \nraceOther                   0.3349309  0.2320411   1.443 0.148905    \nraceOther.Asian             0.1312525  0.2378787   0.552 0.581111    \nraceUnknown                 0.7716858  0.2301287   3.353 0.000799 ***\nraceVietnamese             -0.0043229  0.3071443  -0.014 0.988771    \nraceWhite                   0.6653885  0.2304941   2.887 0.003892 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6300.2  on 5907  degrees of freedom\nResidual deviance: 5790.0  on 5891  degrees of freedom\n  (339 observations deleted due to missingness)\nAIC: 20106\n\nNumber of Fisher Scoring iterations: 5\n\n# Fit Negative Binomial regression with pivoted data\ncrime_race_long &lt;- crime_race_long %&gt;%\n  filter(complete.cases(avg_median_income, race, crime_count))\n\nnb_model &lt;- glm.nb(\n  crime_count ~ avg_median_income * race,\n  data = crime_race_long\n)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = crime_count ~ avg_median_income * race, data = crime_race_long, \n    init.theta = 9.252493725, link = log)\n\nCoefficients: (1 not defined because of singularities)\n                                               Estimate Std. Error z value\n(Intercept)                                   0.1455120  0.4202718   0.346\navg_median_income                            -0.0016531  0.0063198  -0.262\nraceAsian.Indian                             -0.1455120  1.0570790  -0.138\nraceBlack                                     0.7861658  0.4246697   1.851\nraceCambodian                                 0.4754042  1.0460402   0.454\nraceChinese                                  -0.1475022  0.5367116  -0.275\nraceFilipino                                 -0.1663137  0.5407560  -0.308\nraceHawaiian                                 -0.1455120  1.0060103  -0.145\nraceHispanic.Latin.Mexican                    1.0450224  0.4226039   2.473\nraceJapanese                                  0.1871495  0.6428169   0.291\nraceKorean                                    0.1289437  0.4757716   0.271\nraceLaotian                                  -0.0274248  1.0855404  -0.025\nraceOther                                     0.2422653  0.4272637   0.567\nraceOther.Asian                               0.0666385  0.4443021   0.150\nraceUnknown                                   0.7903132  0.4223953   1.871\nraceVietnamese                               -0.1455120  0.6123748  -0.238\nraceWhite                                     0.4589181  0.4231629   1.084\navg_median_income:raceAsian.Indian            0.0016531  0.0115164   0.144\navg_median_income:raceBlack                  -0.0027989  0.0063851  -0.438\navg_median_income:raceCambodian              -0.0043472  0.0145757  -0.298\navg_median_income:raceChinese                 0.0030248  0.0071816   0.421\navg_median_income:raceFilipino                0.0021817  0.0077737   0.281\navg_median_income:raceHawaiian                0.0016531  0.0130268   0.127\navg_median_income:raceHispanic.Latin.Mexican -0.0038425  0.0063534  -0.605\navg_median_income:raceJapanese               -0.0019210  0.0091102  -0.211\navg_median_income:raceKorean                 -0.0001553  0.0069246  -0.022\navg_median_income:raceLaotian                        NA         NA      NA\navg_median_income:raceOther                   0.0011020  0.0063781   0.173\navg_median_income:raceOther.Asian             0.0008324  0.0065657   0.127\navg_median_income:raceUnknown                -0.0003425  0.0063433  -0.054\navg_median_income:raceVietnamese              0.0016531  0.0079050   0.209\navg_median_income:raceWhite                   0.0023250  0.0063396   0.367\n                                             Pr(&gt;|z|)  \n(Intercept)                                    0.7292  \navg_median_income                              0.7937  \nraceAsian.Indian                               0.8905  \nraceBlack                                      0.0641 .\nraceCambodian                                  0.6495  \nraceChinese                                    0.7835  \nraceFilipino                                   0.7584  \nraceHawaiian                                   0.8850  \nraceHispanic.Latin.Mexican                     0.0134 *\nraceJapanese                                   0.7709  \nraceKorean                                     0.7864  \nraceLaotian                                    0.9798  \nraceOther                                      0.5707  \nraceOther.Asian                                0.8808  \nraceUnknown                                    0.0613 .\nraceVietnamese                                 0.8122  \nraceWhite                                      0.2781  \navg_median_income:raceAsian.Indian             0.8859  \navg_median_income:raceBlack                    0.6611  \navg_median_income:raceCambodian                0.7655  \navg_median_income:raceChinese                  0.6736  \navg_median_income:raceFilipino                 0.7790  \navg_median_income:raceHawaiian                 0.8990  \navg_median_income:raceHispanic.Latin.Mexican   0.5453  \navg_median_income:raceJapanese                 0.8330  \navg_median_income:raceKorean                   0.9821  \navg_median_income:raceLaotian                      NA  \navg_median_income:raceOther                    0.8628  \navg_median_income:raceOther.Asian              0.8991  \navg_median_income:raceUnknown                  0.9569  \navg_median_income:raceVietnamese               0.8344  \navg_median_income:raceWhite                    0.7138  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(9.2525) family taken to be 1)\n\n    Null deviance: 4800.1  on 5907  degrees of freedom\nResidual deviance: 4305.8  on 5877  degrees of freedom\nAIC: 19758\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  9.252 \n          Std. Err.:  0.706 \n\n 2 x log-likelihood:  -19694.184 \n\ncrime_race_long &lt;- crime_race_long %&gt;%\n  mutate(predicted_crimes = predict(nb_model, newdata = crime_race_long, type = \"response\"))\n\n\ncolnames(crime_race_long)\n\n[1] \"GEOID\"             \"avg_median_income\" \"geometry\"         \n[4] \"total_crimes\"      \"race\"              \"crime_count\"      \n[7] \"predicted_crimes\" \n\ngrid &lt;- expand.grid(\n  avg_median_income = seq(min(crime_race_long$avg_median_income), max(crime_race_long$avg_median_income), length.out = 100),\n  race = unique(crime_race_long$race)\n)\n\ngrid$predicted_crimes &lt;- predict(nb_model, newdata = grid, type = \"response\")\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nggplot(grid, aes(x = avg_median_income, y = predicted_crimes, color = race)) +\n  geom_line() +\n  labs(\n    title = \"Predicted Crime Counts vs Median Income by Race\",\n    x = \"Average Median Income (in $1,000s)\",\n    y = \"Predicted Crime Counts\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(grid, aes(x = avg_median_income, y = predicted_crimes, color = race)) +\n  geom_line() +\n  facet_wrap(~race, scales = \"free_y\") +  # Facet by race, allowing y-axis to vary by panel\n  labs(\n    title = \"Predicted Crime Counts vs Median Income by Race (Facet by Race)\",\n    x = \"Average Median Income (in $1,000s)\",\n    y = \"Predicted Crime Counts\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet labels\n    legend.position = \"none\"              # Hide legend if redundant\n  )"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html",
    "title": "blog post 5",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project\n\nLoading required package: viridisLite\n\n\nReading layer `8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu' from data source `/Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project/LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 6359592 ymin: 1715035 xmax: 6514633 ymax: 1945515\nProjected CRS: NAD83 / California zone 5 (ftUS)\n\n\nGetting data from the 2016-2020 5-year ACS\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n# Pull median income data for Los Angeles\nincome_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(median_income = \"B19013_001\"),\n  state = \"CA\",\n  county = \"Los Angeles\",\n  year = 2020,\n  geometry = TRUE\n)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Prepare the data for mapping (ensure CRS consistency)\nincome_data &lt;- st_transform(income_data, crs = st_crs(crime_summary))\n\n# Join income data with crime data (assuming `crime_summary` has neighborhood geometries)\ncrime_income_map &lt;- st_join(crime_summary, income_data, join = st_within)\n\n# Filter out areas with missing income data (if needed)\ncrime_income_map &lt;- crime_income_map %&gt;% filter(!is.na(estimate))\n\n# Ensure geometries are valid and filter out empty or invalid geometries\n# Filter out geometries with coordinates at (0, 0) or zero degrees west (longitude = 0)\ncrime_income_map &lt;- crime_income_map %&gt;%\n  filter(!st_is_empty(geometry)) %&gt;%  # Ensure geometry is not empty\n  filter(!sapply(geometry, function(geom) {\n    coords &lt;- st_coordinates(geom)\n    return(any(coords[,1] == 0 & coords[,2] == 0) || any(coords[,1] == 0) || any(coords[,2] == 0))\n  }))\n\nggplot() +\n  geom_sf(data = income_data, aes(fill = estimate), color = NA) +  # Median income layer\n  geom_sf(data = crime_summary, aes(color = avg_income), size = 2, alpha = 0.7) +  # Crime data layer\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  labs(\n    title = \"Median Income and Crime Data by Neighborhood in Los Angeles\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Adjust these values to focus on the Los Angeles area\n    ylim = c(33, 35)       # Adjust these values to focus on the Los Angeles area\n  )"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#we-had-combine-the-temp-data-and-data-that-we-are-using-by-using-left-join",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#we-had-combine-the-temp-data-and-data-that-we-are-using-by-using-left-join",
    "title": "blog post 5",
    "section": "We had combine the temp data and data that we are using by using left join",
    "text": "We had combine the temp data and data that we are using by using left join\n\nlatemp &lt;- readRDS(here(\"latemp.rds\"))\nlatemp &lt;- latemp %&gt;%\n  mutate(DATE = as.Date(DATE)) %&gt;%\n  rename(DATE.OCC = DATE)\nlatemp_unique &lt;- latemp %&gt;%\n  distinct(DATE.OCC, .keep_all = TRUE)\ndata_with_temp &lt;- data %&gt;%\n  left_join(latemp_unique, by = \"DATE.OCC\")"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#find-the-top-20-temperture-that-mostly-to-occur-crime",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#find-the-top-20-temperture-that-mostly-to-occur-crime",
    "title": "blog post 5",
    "section": "Find the top 20 temperture that mostly to occur crime",
    "text": "Find the top 20 temperture that mostly to occur crime\n\ntop_20_temps &lt;- data_with_temp %&gt;%\n  count(TAVG, sort = TRUE) %&gt;%\n  top_n(20, n)\n\ntop_20_temps\n\n# A tibble: 20 × 2\n    TAVG     n\n   &lt;int&gt; &lt;int&gt;\n 1    60   754\n 2    58   737\n 3    63   660\n 4    67   654\n 5    65   589\n 6    59   584\n 7    62   584\n 8    69   583\n 9    61   564\n10    71   527\n11    66   510\n12    64   498\n13    70   483\n14    72   473\n15    74   461\n16    55   435\n17    57   435\n18    56   431\n19    68   370\n20    73   361"
  },
  {
    "objectID": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#ggplot-for-temp-vs-crime-occurs",
    "href": "posts/2024-11-10-blog-post-5-/blog-post-5-.html#ggplot-for-temp-vs-crime-occurs",
    "title": "blog post 5",
    "section": "ggplot for temp vs crime occurs",
    "text": "ggplot for temp vs crime occurs\n\nggplot(data_with_temp, aes(x = TAVG)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Crime Count by Temperature\",\n       x = \"Average Temperature (TAVG)\",\n       y = \"Crime Count\") +\n  theme_minimal()\n\nWarning: Removed 29 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe histogram shows that crime counts are higher in the temperature range of approximately 55°F to 75°F. Within this range, the frequency of crimes appears to be fairly consistent, with a peak around 60°F to 70°F. As temperatures exceed 75°F, there is a noticeable decline in crime counts. This suggests that extremely warm days may be associated with lower crime activity. Similarly, at lower temperatures (below 50°F), there are also fewer crime incidents. This could be due to people staying indoors more on colder days, leading to less social interaction and possibly fewer opportunities for crime."
  },
  {
    "objectID": "posts/2024-11-13-blog-post-6/blog-post-6.html",
    "href": "posts/2024-11-13-blog-post-6/blog-post-6.html",
    "title": "blog post 6",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project\n\nLoading required package: viridisLite\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\nGetting data from the 2016-2020 5-year ACS\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`."
  },
  {
    "objectID": "posts/2024-11-13-blog-post-6/blog-post-6.html#crime-in-each-neighborhood",
    "href": "posts/2024-11-13-blog-post-6/blog-post-6.html#crime-in-each-neighborhood",
    "title": "blog post 6",
    "section": "Crime in each neighborhood",
    "text": "Crime in each neighborhood\n\n# Define broader crime categories based on `Crm.Cd.Desc`\ncrime_with_neighborhoods &lt;- crime_with_neighborhoods %&gt;%\n  mutate(Crime_Category = case_when(\n    str_detect(Crm.Cd.Desc, \"BURGLARY|THEFT|VANDALISM|EMBEZZLEMENT|FORGERY|SHOPLIFTING\") ~ \"Property Crime\",\n    str_detect(Crm.Cd.Desc, \"ASSAULT|ROBBERY|HOMICIDE|KIDNAPPING|SEXUAL ASSAULT\") ~ \"Violent Crime\",\n    str_detect(Crm.Cd.Desc, \"DRUG\") ~ \"Drug Offense\",\n    str_detect(Crm.Cd.Desc, \"TRAFFIC|DRIVING|DUI|FAILURE TO YIELD\") ~ \"Traffic Violation\",\n    TRUE ~ \"Other\"\n  ))\ncrime_summary &lt;- crime_with_neighborhoods %&gt;%\n  group_by(AREA.NAME, Crime_Category) %&gt;%\n  summarize(total_crimes = n(), .groups = \"drop\")\nggplot(crime_summary, aes(x = reorder(AREA.NAME, total_crimes), y = total_crimes, fill = Crime_Category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Crime Categories by Neighborhood\",\n    x = \"Neighborhood\",\n    y = \"Total Crimes\",\n    fill = \"Crime Category\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nggplot(crime_summary, aes(x = reorder(AREA.NAME, total_crimes), y = total_crimes, fill = Crime_Category)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(\n    title = \"Proportion of Crime Categories by Neighborhood\",\n    x = \"Neighborhood\",\n    y = \"Proportion\",\n    fill = \"Crime Category\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThese graphs tell a compelling story about crime patterns in neighborhoods. Property crimes are a widespread issue, while violent crimes are relatively localized. Hollywood and similar areas show distinctive crime patterns, suggesting localized factors influencing crime dynamics.The 77th Street neighborhood stands out in both graphs due to the almost equal distribution of Property Crime and Other Crime categories. This is unusual compared to most neighborhoods where Property Crime significantly dominates.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nset.seed(123) \n\nkmeans_data &lt;- crime_with_income %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarize(\n    avg_income = mean(estimate, na.rm = TRUE),  \n    total_crimes = n()  \n  ) %&gt;%\n  na.omit() \nkmeans_data_numeric &lt;- kmeans_data %&gt;%\n  st_drop_geometry() %&gt;%  \n  select(avg_income, total_crimes)  \nkmeans_result &lt;- kmeans(kmeans_data_numeric, centers = 3)\nkmeans_data &lt;- kmeans_data %&gt;%\n  mutate(cluster = factor(kmeans_result$cluster))\n\n\nggplot(kmeans_data, aes(x = avg_income, y = total_crimes, color = cluster)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(\n    title = \"clustering neighborhoods by income and crime\",\n    x = \"Median Income\",\n    y = \"Total Crimes\",\n    color = \"Cluster\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\ncluster 1: moderate crime counts and moderate income levels cluster 2: higher income levels and lower crime counts cluster 3: high crime counts and lower income levels\n\ncluster_centers &lt;- as.data.frame(kmeans_result$centers)\n\nggplot(kmeans_data, aes(x = avg_income, y = total_crimes, color = cluster)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_point(data = cluster_centers, aes(x = avg_income, y = total_crimes), size = 5, shape = 17, color = \"black\") + \n  labs(\n    title = \"clustering neighborhoods by income and crime with centroids\",\n    x = \"Median Income\",\n    y = \"Total Crimes\",\n    color = \"Cluster\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nthe black triangles represent the average values of income and crime for each cluster. this shows that crime counts decrease as median income increases."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html",
    "title": "Blog post 3",
    "section": "",
    "text": "The following code blocks can be found in the scripts file under Sean test scripts for a complete R file for testing. This will be higher level descriptions."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#background-information",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#background-information",
    "title": "Blog post 3",
    "section": "",
    "text": "The following code blocks can be found in the scripts file under Sean test scripts for a complete R file for testing. This will be higher level descriptions."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#load-required-libraries",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#load-required-libraries",
    "title": "Blog post 3",
    "section": "1. Load Required Libraries",
    "text": "1. Load Required Libraries\n\n# Load required libraries\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project"
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#clean-dataset-for-readability-and-ease-of-use",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#clean-dataset-for-readability-and-ease-of-use",
    "title": "Blog post 3",
    "section": "2. Clean Dataset for Readability and Ease of Use",
    "text": "2. Clean Dataset for Readability and Ease of Use\n\ndata &lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndata &lt;- readRDS(\"filtered_data_copy.rds\")\n\n\n# Create a tibble with the mapping of descent codes\ndescent_codes &lt;- tribble(\n  ~Descent_Code, ~Descent_Description,\n  \"A\", \"Other Asian\",\n  \"B\", \"Black\",\n  \"C\", \"Chinese\",\n  \"D\", \"Cambodian\",\n  \"F\", \"Filipino\",\n  \"G\", \"Guamanian\",\n  \"H\", \"Hispanic/Latin/Mexican\",\n  \"I\", \"American Indian/Alaskan Native\",\n  \"J\", \"Japanese\",\n  \"K\", \"Korean\",\n  \"L\", \"Laotian\",\n  \"O\", \"Other\",\n  \"P\", \"Pacific Islander\",\n  \"S\", \"Samoan\",\n  \"U\", \"Hawaiian\",\n  \"V\", \"Vietnamese\",\n  \"W\", \"White\",\n  \"X\", \"Unknown\",\n  \"Z\", \"Asian Indian\"\n)\n\n# Join the tibble with the main dataset by Vict.Descent\ndata_clean &lt;- data %&gt;%\n  left_join(descent_codes, by = c(\"Vict.Descent\" = \"Descent_Code\"))\n\n# Replace unmatched or missing descriptions with \"Unknown\"\ndata_clean$Descent_Description[is.na(data_clean$Descent_Description)] &lt;- \"Unknown\"\n\n# Drop irrelevant columns with many NAs\ndata_clean &lt;- data_clean[, !(names(data_clean) %in% c(\"Crm.Cd.2\", \"Crm.Cd.3\", \"Crm.Cd.4\", \"Weapon.Used.Cd\", \"Weapon.Desc\", \"Cross.Street\"))]\ndata_clean$DATE.OCC &lt;- as.POSIXct(data_clean$DATE.OCC, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n# Extract the 'month' column in \"YYYY-MM\" format\ndata_clean$month &lt;- format(data_clean$DATE.OCC, \"%Y-%m\")\n\n# Check the cleaned data\nnrow(data_clean)\n\n[1] 12500\n\nstr(data_clean)\n\ntibble [12,500 × 25] (S3: tbl_df/tbl/data.frame)\n $ DR_NO              : int [1:12500] 200604124 200218223 210105387 201314683 201610483 200907309 200905047 200111368 201508876 201111487 ...\n $ Date.Rptd          : chr [1:12500] \"01/03/2020 12:00:00 AM\" \"12/05/2020 12:00:00 AM\" \"02/02/2021 12:00:00 AM\" \"07/29/2020 12:00:00 AM\" ...\n $ DATE.OCC           : POSIXct[1:12500], format: \"2020-01-03\" \"2020-12-05\" ...\n $ TIME.OCC           : int [1:12500] 400 100 1550 1035 1455 1220 1230 2007 417 1235 ...\n $ AREA               : int [1:12500] 6 2 1 13 16 9 9 1 15 11 ...\n $ AREA.NAME          : chr [1:12500] \"Hollywood\" \"Rampart\" \"Central\" \"Newton\" ...\n $ Rpt.Dist.No        : int [1:12500] 648 235 152 1383 1695 994 915 159 1549 1115 ...\n $ Part.1.2           : int [1:12500] 1 1 2 2 2 1 2 2 1 2 ...\n $ Crm.Cd             : int [1:12500] 310 510 649 740 354 310 745 940 310 662 ...\n $ Crm.Cd.Desc        : chr [1:12500] \"BURGLARY\" \"VEHICLE - STOLEN\" \"DOCUMENT FORGERY / STOLEN FELONY\" \"VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)\" ...\n $ Mocodes            : chr [1:12500] \"0325 0601 1601 1606\" \"\" \"1214 0923\" \"0329 1307\" ...\n $ Vict.Age           : int [1:12500] 0 0 0 0 26 48 57 37 0 60 ...\n $ Vict.Sex           : chr [1:12500] \"X\" \"\" \"X\" \"M\" ...\n $ Vict.Descent       : chr [1:12500] \"X\" \"\" \"X\" \"W\" ...\n $ Premis.Cd          : int [1:12500] 210 101 602 122 501 501 157 502 222 501 ...\n $ Premis.Desc        : chr [1:12500] \"RESTAURANT/FAST FOOD\" \"STREET\" \"BANK\" \"VEHICLE, PASSENGER/TRUCK\" ...\n $ Status             : chr [1:12500] \"IC\" \"IC\" \"IC\" \"IC\" ...\n $ Status.Desc        : chr [1:12500] \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" \"Invest Cont\" ...\n $ Crm.Cd.1           : int [1:12500] 310 510 649 740 354 310 745 940 310 662 ...\n $ LOCATION           : chr [1:12500] \"5400 W  SUNSET                       BL\" \"2200 W  TEMPLE                       ST\" \"800    WILSHIRE                     BL\" \"6200 S  BROADWAY\" ...\n $ LAT                : num [1:12500] 34.1 34.1 34 34 34.2 ...\n $ LON                : num [1:12500] -118 -118 -118 -118 -118 ...\n $ year               : num [1:12500] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n $ Descent_Description: chr [1:12500] \"Unknown\" \"Unknown\" \"Unknown\" \"White\" ...\n $ month              : chr [1:12500] \"2020-01\" \"2020-12\" \"2020-10\" \"2020-07\" ...\n\n\nThis was done in an effort to make the data more readable and cleaner. It gets rid of columns that had a lot of N/A’s in the column and changes the date column to remove the 12:00 AM that was previously there and replacing it with a “DATE OCC” column that was not initially there to make the data easier to change.\nIt also creates another column that takes the Vict.Descent column and uses the codes from a tibble to create another column called “Descent_Code” in order to get a more readable information. Then, I checked the data."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#data-visualization",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#data-visualization",
    "title": "Blog post 3",
    "section": "3. Data Visualization",
    "text": "3. Data Visualization"
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-locations-with-crime",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-locations-with-crime",
    "title": "Blog post 3",
    "section": "3.1 Top Locations with Crime",
    "text": "3.1 Top Locations with Crime\n\nggplot(data_clean, aes(x = reorder(AREA.NAME, AREA.NAME, function(x) -length(x)))) +\n  geom_bar(fill = \"steelblue\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Top Locations with Highest Crime\", x = \"Area\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis depicts the locations for the places with the highest crime count. We previously did this last week but I wasn’t sure what the graph produced was so I recreated it. Same data as last week but a better visualization. Personally, nothing to note here, it seems like crime is distributd pretty evenly throughout the ditricts. Can be something to be mentioned in the final project."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-frequency-over-time",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-frequency-over-time",
    "title": "Blog post 3",
    "section": "3.2 Crime Frequency Over Time",
    "text": "3.2 Crime Frequency Over Time\n\nggplot(data_clean, aes(x = month)) +\n  geom_bar(fill = \"darkred\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Crime Frequency Over Time\", x = \"Month\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis is a bit hard to read but I did find it interesting that there was a spike in crime in 2024 January-February. May look into further for potential findings on why that was. Otherwise, crime seems decently evenly distributed by time."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-counts-by-descent-description",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#crime-counts-by-descent-description",
    "title": "Blog post 3",
    "section": "3.3 Crime Counts by Descent Description",
    "text": "3.3 Crime Counts by Descent Description\n\nggplot(data_clean, aes(x = reorder(Descent_Description, Descent_Description, function(x) -length(x)))) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Crime Counts by Descent Description\", x = \"Descent Category\", y = \"Count\")\n\n\n\n\n\n\n\n\nInteresting that Hispanics represent the largest amount of crime counts by description. Even more interesting, is that Whites are second given different American stereotypes. May look into doing a correlation chart on this."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-20-most-frequent-crime-types",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#top-20-most-frequent-crime-types",
    "title": "Blog post 3",
    "section": "3.4 Top 20 Most Frequent Crime Types",
    "text": "3.4 Top 20 Most Frequent Crime Types\n\n# Calculate the frequency of each crime type\ntop_crime_types &lt;- data_clean %&gt;%\n  count(Crm.Cd.Desc, sort = TRUE) %&gt;%\n  top_n(20, n)\n\nggplot(top_crime_types, aes(x = reorder(Crm.Cd.Desc, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Top 20 Most Frequent Crime Types\", x = \"Crime Type\", y = \"Count\")\n\n\n\n\n\n\n\n\nInitially looked at all types of crime but this was too large a list so it was condensed down to the above chart with the top 20 most frequent. May look into condensing some of these categories such as burglary and robbery and look into the difference between those two crimes to see if they can be condensed. I really want to treat the $950 above and below as the same category. Still, it is interesting to see that a lot of people are still willing to commit felonies after the revision of the burglary codes. The difference between the $950 over and under is not too much."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#monthly-crime-frequency-by-descent",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#monthly-crime-frequency-by-descent",
    "title": "Blog post 3",
    "section": "3.5 Monthly Crime Frequency by Descent",
    "text": "3.5 Monthly Crime Frequency by Descent\n\nggplot(data_clean, aes(x = month, fill = Descent_Description)) +\n  geom_bar(position = \"stack\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Monthly Crime Frequency by Descent\", x = \"Month\", y = \"Count\")\n\n\n\n\n\n\n\n\nThis is a confusing chart but an interesting visualization. Could be used if it considered most frequent to least frequent but we will see if we can incorporate this somehow."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#next-steps",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#next-steps",
    "title": "Blog post 3",
    "section": "4. Next Steps",
    "text": "4. Next Steps\nMay look into saving the cleaned data as a seperate rds file depending on groupmate feedback. We will see. I will sleep on it.\nNeed to look at more charts like the places description to see if there are any indicators there. Also need to take a look at the time description to see which hours people are committing crime. I’m not doing anymore though at least not this week.\nLastly, look into formulating an idea for the direction of the project and any potential uses of the “lm()” function.\n##5. Charts about places and time that committing crime\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf&lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndf&lt;- readRDS(\"filtered_data_copy.rds\")\n\ndf &lt;- df %&gt;% select(-Mocodes) \n\nWe take out Mocodes since it’s a kind of code that use in police department. It’s hard for us to analyze some useful Information.\n\ndf &lt;- df %&gt;%\n  mutate(\n    TIME.OCC = sprintf(\"%04d\", TIME.OCC),  \n    TIME.OCC = format(strptime(TIME.OCC, format = \"%H%M\"), \"%H:%M\")  \n  )\n\nWe transform the type of TIME.OCC to become HH:MM instead of numerical. It’s easier for us to read because some of the time like 00:01 will be handle as 1 in the previous TIME.OCC column. We plus in the time in graph to see when would have the most case occurs.\n\nggplot(df, aes(x = TIME.OCC)) +\n  geom_bar(stat = \"count\", fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Count of TIME.OCC\", x = \"Time of Occurrence\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can’t see a lot of pattern because the graph is not sorted as hourly. We will take out HH from HH:MM and find the count of case to see in what hour in a day would have most case occurs.\n\ndf &lt;- df %&gt;%\n  mutate(Hour = substr(TIME.OCC, 1, 2))\nhead(df)\n\n# A tibble: 6 × 29\n   DR_NO Date.Rptd DATE.OCC TIME.OCC  AREA AREA.NAME Rpt.Dist.No Part.1.2 Crm.Cd\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;  &lt;int&gt;\n1 2.01e8 01/03/20… 01/03/2… 04:00        6 Hollywood         648        1    310\n2 2.00e8 12/05/20… 12/05/2… 01:00        2 Rampart           235        1    510\n3 2.10e8 02/02/20… 10/02/2… 15:50        1 Central           152        2    649\n4 2.01e8 07/29/20… 07/29/2… 10:35       13 Newton           1383        2    740\n5 2.02e8 06/28/20… 06/25/2… 14:55       16 Foothill         1695        2    354\n6 2.01e8 03/12/20… 03/10/2… 12:20        9 Van Nuys          994        1    310\n# ℹ 20 more variables: Crm.Cd.Desc &lt;chr&gt;, Vict.Age &lt;int&gt;, Vict.Sex &lt;chr&gt;,\n#   Vict.Descent &lt;chr&gt;, Premis.Cd &lt;int&gt;, Premis.Desc &lt;chr&gt;,\n#   Weapon.Used.Cd &lt;int&gt;, Weapon.Desc &lt;chr&gt;, Status &lt;chr&gt;, Status.Desc &lt;chr&gt;,\n#   Crm.Cd.1 &lt;int&gt;, Crm.Cd.2 &lt;int&gt;, Crm.Cd.3 &lt;int&gt;, Crm.Cd.4 &lt;int&gt;,\n#   LOCATION &lt;chr&gt;, Cross.Street &lt;chr&gt;, LAT &lt;dbl&gt;, LON &lt;dbl&gt;, year &lt;dbl&gt;,\n#   Hour &lt;chr&gt;\n\nhourly_counts &lt;- df %&gt;%\n  group_by(Hour) %&gt;%\n  summarise(Count = n())\nggplot(hourly_counts, aes(x = Hour, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Occurrences by Hour\", x = \"Hour of the Day\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe answer is actually suprising because people thinks that mid night would be the time that will have most case. But it seems that noon time is would be the one that have the most case. It may be the reason that people may not realize and call the police after they wake up in the morning.\nThis is the most updated version."
  },
  {
    "objectID": "posts/2024-10-23-blog-post-3/blog-post-3.html#analyze-the-relationship-between-top-crimes-and-specific-areas.",
    "href": "posts/2024-10-23-blog-post-3/blog-post-3.html#analyze-the-relationship-between-top-crimes-and-specific-areas.",
    "title": "Blog post 3",
    "section": "6. Analyze the relationship between top crimes and specific areas.",
    "text": "6. Analyze the relationship between top crimes and specific areas.\n##6.1 analyze the highest crime in each of the top 10 areas.\n\n# Calculate total crime count by area and select top 10 areas\n\ndata &lt;- readRDS(here(\"filtered_data_copy.rds\"))\n\ndata &lt;- readRDS(\"filtered_data_copy.rds\")\n\ncrime_area_summary &lt;- data %&gt;%\n  group_by(AREA.NAME, Crm.Cd.Desc) %&gt;%\n  summarise(Crime_Count = n())\n\n`summarise()` has grouped output by 'AREA.NAME'. You can override using the\n`.groups` argument.\n\ntop_10_areas &lt;- crime_area_summary %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarise(Total_Crimes = sum(Crime_Count)) %&gt;%\n  arrange(desc(Total_Crimes)) %&gt;%\n  top_n(10, Total_Crimes) %&gt;%\n  pull(AREA.NAME)  \n\n\n# Filter for top 10 areas\ntop_10_area_crimes &lt;- crime_area_summary %&gt;%\n  filter(AREA.NAME %in% top_10_areas)\n\n# Find the most common crime type in each top area\nhighest_crime_per_area &lt;- top_10_area_crimes %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  filter(Crime_Count == max(Crime_Count)) %&gt;%\n  select(AREA.NAME, Crm.Cd.Desc, Crime_Count) %&gt;%\n  arrange(desc(Crime_Count))\n\n\n# plot showing the highest crime in each of the top 10 areas\nggplot(highest_crime_per_area, aes(x = reorder(AREA.NAME, -Crime_Count), y = Crime_Count, fill = Crm.Cd.Desc)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Common Crime Type in Each of the Top 10 Areas\",\n       x = \"Area\",\n       y = \"Crime Count\",\n       fill = \"Crime Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis helps identifying which crime types are dominant in high-crime areas, which could be usedful for targeted policy-making. it is worthy noticing that vehicle stolen us a widespread issue across several high crime areas. burglary is the top crime in wilshire, showing a unique pattern of property crimes in that area compared to the others.\n##6.2 analyze which area is the most prevalent for each of the top 10 crimes\n\n# Identify the top 10 most frequent crime types\ntop_10_crimes &lt;- data_clean %&gt;%\n  count(Crm.Cd.Desc, sort = TRUE) %&gt;%\n  top_n(10, n) %&gt;%\n  pull(Crm.Cd.Desc) \n\n\n# Filter for the top 10 crimes\ntop_10_crime_areas &lt;- crime_area_summary %&gt;%\n  filter(Crm.Cd.Desc %in% top_10_crimes)\n\n# Find the most prevalent area for each top crime\nmost_prevalent_area_per_crime &lt;- top_10_crime_areas %&gt;%\n  group_by(Crm.Cd.Desc) %&gt;%\n  filter(Crime_Count == max(Crime_Count)) %&gt;%\n  select(Crm.Cd.Desc, AREA.NAME, Crime_Count) %&gt;%\n  arrange(desc(Crime_Count))\n\n\n# plot showing the most prevalent area for each of the top 10 crimes\nggplot(most_prevalent_area_per_crime, aes(x = reorder(Crm.Cd.Desc, -Crime_Count), y = Crime_Count, fill = AREA.NAME)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Most Prevalent Area for Each of the Top 10 Crimes\",\n       x = \"Crime Type\",\n       y = \"Crime Count\",\n       fill = \"Area\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nvehicle stolen and burglary from vehicle are particulary prevalent in 77th street and central. assault with deadly weapon, aggravated assault and battery simple assault also occur very frequently in 77th street. theft of identity is most prevalent in N Hollywood, which shows a different crime pattern in this area\ncombining with these two charts together, vehicle theft and burglary from vehicles dominate as the most frequent crime types in multiple areas, especially in 77th street and central. the central area has a high incidence of various crimes, which may indicating that the central area faces broader challenges.\n\n# Convert the date column to Date format if not already in that format\ndata$DATE.OCC &lt;- as.POSIXct(data$DATE.OCC, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n# Create an age group column\ndata &lt;- data %&gt;%\n  mutate(Age_Group = case_when(\n    Vict.Age &lt; 18 ~ \"Under 18\",\n    Vict.Age &gt;= 18 & Vict.Age &lt; 30 ~ \"18-29\",\n    Vict.Age &gt;= 30 & Vict.Age &lt; 45 ~ \"30-44\",\n    Vict.Age &gt;= 45 & Vict.Age &lt; 60 ~ \"45-59\",\n    Vict.Age &gt;= 60 ~ \"60+\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# Replace missing or \"X\" gender values with \"Unknown\"\ndata &lt;- data %&gt;%\n  mutate(Vict.Sex = ifelse(is.na(Vict.Sex) | Vict.Sex == \"X\", \"Unknown\", Vict.Sex))\n\n# Plot the distribution of crimes by age group and gender, including unknown gender\nggplot(data, aes(x = Age_Group, fill = Vict.Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Distribution of Crimes by Victim's Age Group and Gender\",\n       x = \"Age Group\",\n       y = \"Number of Crimes\",\n       fill = \"Gender\") +\n  scale_fill_manual(values = c(\"F\" = \"green\", \"M\" = \"purple\", \"H\" = \"cyan\", \"Unknown\" = \"orange\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe highest number of crimes appears to target females under 18, represented by the red bar in the “Under 18” category. This could indicate that crimes against younger females are a significant issue. Additionally The bar heights suggest that crimes involving male victims increase steadily from younger to older age groups, peaking around 30-44 and 45-59 age brackets. After 60, crime incidence significantly declines for all genders and Younger individuals (under 18 and 18-29) seem to experience a higher number of crimes overall compared to older individuals, indicating that crime incidents decrease with age."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "This dataset reflects incidents of crime in the City of Los Angeles dating back to 2020 which is collected and sourced by the Los Angeles Police Department. The LAPD collects this data as part of routine public safety operations in order to better track crime patterns, allocating resources, and for transparency purposes, ensuring that the public has access to crime data. While looking through the dataset, there are some issues with collecting the data. Since the LAPD records the crime data through reports, there can be several inconsistencies with how crime reports with different officers or stations taking in data as well as failing to account for unreported crime. This would lead to a dataset that doesn’t capture the full picture of crime in LA. With the dataset collecting crime reported from 2020 to the present, there are multiple reasons to think that the sample is biased pertaining to crime. Crime data can be biased due to socioeconomic and geographic factors as crime in lower-income neighborhoods are policed more often and more likely report certain kinds of crime which would skew the data in some ways. Additionally, less policed neighborhoods would have underreported crime data. This data can be used to shape policy in law enforcement to better allocate police resources to prevent crime and be used for research to study socioeconomic facts that play into crime in LA. There has been other research on the same data where they look into more niche subtopics of crime such as domestic violence or crime involving homeless people."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#background-information-oscar",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#background-information-oscar",
    "title": "Blog Post 2",
    "section": "",
    "text": "This dataset reflects incidents of crime in the City of Los Angeles dating back to 2020 which is collected and sourced by the Los Angeles Police Department. The LAPD collects this data as part of routine public safety operations in order to better track crime patterns, allocating resources, and for transparency purposes, ensuring that the public has access to crime data. While looking through the dataset, there are some issues with collecting the data. Since the LAPD records the crime data through reports, there can be several inconsistencies with how crime reports with different officers or stations taking in data as well as failing to account for unreported crime. This would lead to a dataset that doesn’t capture the full picture of crime in LA. With the dataset collecting crime reported from 2020 to the present, there are multiple reasons to think that the sample is biased pertaining to crime. Crime data can be biased due to socioeconomic and geographic factors as crime in lower-income neighborhoods are policed more often and more likely report certain kinds of crime which would skew the data in some ways. Additionally, less policed neighborhoods would have underreported crime data. This data can be used to shape policy in law enforcement to better allocate police resources to prevent crime and be used for research to study socioeconomic facts that play into crime in LA. There has been other research on the same data where they look into more niche subtopics of crime such as domestic violence or crime involving homeless people."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-1-data-reduction",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-1-data-reduction",
    "title": "Blog Post 2",
    "section": "Step 1: Data Reduction",
    "text": "Step 1: Data Reduction\nSince the original dataset was too large, I used a separate R project to filter and reduce the number of rows in the CSV file. After processing, the data was saved as an RDS file to make it easier to work with. Below is the code I used:\n# Convert the date column to year format\ndata$year &lt;- format(as.Date(data$DATE.OCC, format = \"%m/%d/%Y\"), \"%Y\")\ndata$year &lt;- as.numeric(data$year)  # Convert year to numeric\n\n# Filter for the years 2020–2024\nfiltered_years &lt;- data %&gt;%\n  filter(year %in% 2020:2024)\n\n# Split by year and sample up to 2,500 rows per year\nsampled_data &lt;- filtered_years %&gt;%\n  group_split(year) %&gt;%\n  map_df(~ slice_sample(.x, n = min(2500, nrow(.x))))\n\n# View the sampled data\nhead(sampled_data)\n\n# Save the filtered data as an RDS file\nsaveRDS(sampled_data, \"filtered_data.rds\")"
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-2-data-import-for-group-collaboration",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#step-2-data-import-for-group-collaboration",
    "title": "Blog Post 2",
    "section": "Step 2: Data Import for Group Collaboration",
    "text": "Step 2: Data Import for Group Collaboration\nAfter cleaning and sampling the data, I imported the RDS file into the final project so that all group members could access it easily.\n# Example of loading the RDS file\nsampled_data &lt;- readRDS(\"filtered_data.rds\")\nThis approach ensures that our dataset is manageable and ready for further analysis, while also maintaining the integrity of the original data."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-loading-and-exploration-zihao-and-siqi",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-loading-and-exploration-zihao-and-siqi",
    "title": "Blog Post 2",
    "section": "Data Loading and exploration (Zihao and Siqi)",
    "text": "Data Loading and exploration (Zihao and Siqi)\nAfter loading RDS file, we found one interesting facts that we may need to clean out some meaningless value in dataset.\n#ggplot(data,mapping = aes(x=Vict.Age))+geom_histogram(stat=\"count\")\nA lot of officers did not enter the victim’s age, so we can’t do some exploration of the age of victim would most likely to be the target.\nWe also explore the area that would have the most crime.\n#ggplot(data,mapping = aes(x=AREA))+geom_histogram(stat=\"count\")\nIt shows that the top 3 areas are Area 1(Central), Area 12(77th Street), and Area 14(Pacific).\nLast, we also find the count of victims race in the data.\n#ggplot(data,mapping = aes(x=Vict.Descent))+geom_histogram(stat=\"count\")\nIt shows that H (Hispanic/Latin/Mexican)is the most targeted victims. The other few big victims group would be B(Black), O(Other), W(White), and X(Unknown).\nOne thing to be notice is it has the same issue with age that officers did not record 2000 cases’ victims race."
  },
  {
    "objectID": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-for-equity-yawen",
    "href": "posts/2024-10-18-blog-post-2/blog-post-2.html#data-for-equity-yawen",
    "title": "Blog Post 2",
    "section": "Data for Equity (Yawen)",
    "text": "Data for Equity (Yawen)\nTransparency Transparency involves being open about the context of the data and the decisions made during analysis. This includes being clear about how the crime data was collected, what factors are included, and any potential biases present. For example, one potential bias could be over-policing or under-policing in certain areas, which may lead to negative analysis if not acknowledged. Over-policing in a neighborhood could result in a higher number of reported crimes, not necessarily because there is more crime, but due to increased law enforcement presence. Conversely, under-policing in other areas may result in under-reporting, giving a false sense of safety. Both situations can harm residents by reinforcing stereotypes by labeling certain neighborhoods as “high crime”. Adhering to transparency would also involve disclosing any missing data, clarifying that the dataset only reflects reported crimes, and possibly under-reports certain types of crime. Justice Justice is the commitment to the fair distribution of burdens and benefits among people. In our dataset, this means ensuring that data collection and analysis do not harm individuals or communities. Crime data, if not handled carefully, can expose people to risks such as privacy and safety. One way to minimize these risks is to ensure that personally identifiable information, such as names, social security numbers or dates of birth is not included in the dataset. Additionally, the inclusion of specific crime locations in this dataset, could unfairly label certain areas as “high crime”, producing negative perceptions. To promote justice, LAPD could engage with the community residents by holding listening sessions to learn what data the community thinks are relevant to improve their lives. Potential limitations One limitation of the analysis could be the risk of reinforcing stereotypes. Crime data, particularly when aggregated, might suggest that certain neighborhoods are inherently more “criminal” or that specific racial groups are prone to crime, without considering systemic factors such as poverty."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-11-26-blog-post-7/blog-post-7.html",
    "href": "posts/2024-11-26-blog-post-7/blog-post-7.html",
    "title": "blog post 7",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nhere() starts at /Users/oscarmo/ma415/ma-4615-fa24-final-project-group-4\n\nLoading required package: viridisLite\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\nGetting data from the 2016-2020 5-year ACS\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n# Update visualizations\nggplot(crime_summary_lapd, aes(x = avg_income, y = total_crimes)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Crime vs Median Income (LAPD Jurisdiction)\", x = \"Median Income\", y = \"Total Crimes\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  # Median income layer\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  labs(\n    title = \"Median Income within LAPD Jurisdiction\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Focus on the Los Angeles area\n    ylim = c(33, 35)       # Focus on the Los Angeles area\n  )\n\n\n\n\n\n\n\n\n\ncrime_within_lapd &lt;- st_transform(crime_within_lapd, st_crs(economic_data_lapd))\n\nggplot() +\n  # Median income heatmap\n  geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  \n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n  scale_color_viridis_c(option = \"inferno\", na.value = \"grey50\") +\n  # Overlay crime points\n  geom_sf(data = crime_within_lapd, aes(), color = \"black\", shape = 4, size = 0.5, alpha = 0.7) +\n  labs(\n    title = \"Median Income and Crime Data within LAPD Jurisdiction\",\n    fill = \"Median Income\",\n    color = \"Average Income\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  coord_sf(\n    xlim = c(-119, -117),  # Focus on the Los Angeles area\n    ylim = c(33, 35)       # Focus on the Los Angeles area\n  )\n\n\n\n\n\n\n\n#Good place to use shinylive here. Can filter by year. \n\n# crime_filtered &lt;- crime_within_lapd %&gt;%\n#   filter(year == 2021)\n# ggplot() +\n#   geom_sf(data = economic_data_lapd, aes(fill = estimate), color = NA) +  \n#   scale_fill_viridis_c(option = \"magma\", na.value = \"grey50\") +\n#   geom_sf(data = crime_filtered, aes(), color = \"black\", shape = 4, size = 0.5, alpha = 0.7) +\n#   labs(\n#     title = \"Median Income and 2020 Crime Data within LAPD Jurisdiction\",\n#     fill = \"Median Income\"\n#   ) +\n#   theme_minimal() +\n#   coord_sf(xlim = c(-119, -117), ylim = c(33, 35))\n\n#Look at data utilizing a poisson distribution model. \n# Fit a Poisson regression model\n\n\ncrime_summary_race &lt;- crime_with_income %&gt;%\n  group_by(GEOID, Descent_Description) %&gt;%\n  summarize(\n    total_crimes = n(),  # Count crimes for each GEOID and Descent_Description group\n    avg_median_income = mean(estimate, na.rm = TRUE)\n  ) %&gt;%\n  pivot_wider(\n    names_from = Descent_Description,\n    values_from = total_crimes,\n    values_fill = 0  # Fill missing values with 0 for crimes\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    total_crimes = sum(c_across(-c(GEOID, avg_median_income, geometry)), na.rm = TRUE)  # Sum only the crime columns\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'GEOID'. You can override using the\n`.groups` argument.\n\n\n\ncrime_summary_race &lt;- crime_summary_race %&gt;%\n  rename_with(~ make.names(.), everything())\n\nbivar_poisson_model &lt;- glm(\n  total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + White + Black + Other + \n    Other.Asian + Unknown + Japanese + Korean + Filipino + Chinese + Hawaiian + \n    Asian.Indian + American.Indian.Alaskan.Native + Cambodian + Vietnamese + Laotian,\n  family = poisson(link = \"log\"),\n  data = crime_summary_race\n)\nsummary(bivar_poisson_model)\n\n\nCall:\nglm(formula = total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + \n    White + Black + Other + Other.Asian + Unknown + Japanese + \n    Korean + Filipino + Chinese + Hawaiian + Asian.Indian + American.Indian.Alaskan.Native + \n    Cambodian + Vietnamese + Laotian, family = poisson(link = \"log\"), \n    data = crime_summary_race)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     5.342e-01  2.044e-02  26.133   &lt;2e-16 ***\navg_median_income              -2.736e-06  2.547e-07 -10.742   &lt;2e-16 ***\nHispanic.Latin.Mexican          1.655e-01  2.937e-03  56.356   &lt;2e-16 ***\nWhite                           2.038e-01  5.124e-03  39.780   &lt;2e-16 ***\nBlack                           1.700e-01  4.936e-03  34.437   &lt;2e-16 ***\nOther                           1.790e-01  1.227e-02  14.593   &lt;2e-16 ***\nOther.Asian                     2.020e-02  4.523e-02   0.447   0.6552    \nUnknown                         1.032e-01  1.612e-03  64.060   &lt;2e-16 ***\nJapanese                       -3.522e-02  1.605e-01  -0.219   0.8263    \nKorean                          8.766e-02  7.250e-02   1.209   0.2266    \nFilipino                       -1.699e-01  1.118e-01  -1.519   0.1287    \nChinese                         1.525e-01  8.545e-02   1.784   0.0744 .  \nHawaiian                       -2.246e-01  3.782e-01  -0.594   0.5525    \nAsian.Indian                   -1.029e-01  3.336e-01  -0.309   0.7577    \nAmerican.Indian.Alaskan.Native -1.647e-01  1.950e-01  -0.845   0.3984    \nCambodian                      -2.265e-02  3.162e-01  -0.072   0.9429    \nVietnamese                     -2.529e-01  1.965e-01  -1.287   0.1980    \nLaotian                        -3.387e-01  1.000e+00  -0.339   0.7348    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6173.9  on 5564  degrees of freedom\nResidual deviance: 2074.0  on 5547  degrees of freedom\n  (319 observations deleted due to missingness)\nAIC: 15858\n\nNumber of Fisher Scoring iterations: 6\n\nbivar_nb_model &lt;- glm.nb(total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + White + Black + Other + \n    Other.Asian + Unknown + Japanese + Korean + Filipino + Chinese + Hawaiian + \n    Asian.Indian + American.Indian.Alaskan.Native + Cambodian + Vietnamese + Laotian, data = crime_summary_race)\n\nWarning in glm.nb(total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + :\nalternation limit reached\n\nsummary(bivar_nb_model)\n\n\nCall:\nglm.nb(formula = total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + \n    White + Black + Other + Other.Asian + Unknown + Japanese + \n    Korean + Filipino + Chinese + Hawaiian + Asian.Indian + American.Indian.Alaskan.Native + \n    Cambodian + Vietnamese + Laotian, data = crime_summary_race, \n    init.theta = 24.74819092, link = log)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     2.985e-01  2.278e-02  13.104  &lt; 2e-16 ***\navg_median_income              -1.531e-06  2.654e-07  -5.769 7.95e-09 ***\nHispanic.Latin.Mexican          2.251e-01  4.221e-03  53.326  &lt; 2e-16 ***\nWhite                           2.348e-01  6.021e-03  38.992  &lt; 2e-16 ***\nBlack                           2.165e-01  6.348e-03  34.107  &lt; 2e-16 ***\nOther                           2.142e-01  1.342e-02  15.957  &lt; 2e-16 ***\nOther.Asian                     1.222e-01  4.625e-02   2.641  0.00826 ** \nUnknown                         1.757e-01  2.853e-03  61.570  &lt; 2e-16 ***\nJapanese                        5.779e-02  1.665e-01   0.347  0.72856    \nKorean                          1.780e-01  7.480e-02   2.380  0.01733 *  \nFilipino                       -3.709e-02  1.146e-01  -0.324  0.74630    \nChinese                         1.953e-01  8.929e-02   2.187  0.02874 *  \nHawaiian                       -8.249e-02  3.873e-01  -0.213  0.83136    \nAsian.Indian                    9.135e-03  3.431e-01   0.027  0.97876    \nAmerican.Indian.Alaskan.Native -1.634e-02  1.994e-01  -0.082  0.93470    \nCambodian                       9.126e-02  3.242e-01   0.282  0.77831    \nVietnamese                     -1.173e-01  2.009e-01  -0.584  0.55944    \nLaotian                        -1.891e-01  1.020e+00  -0.185  0.85294    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(24.7489) family taken to be 1)\n\n    Null deviance: 5477.7  on 5564  degrees of freedom\nResidual deviance: 1349.0  on 5547  degrees of freedom\n  (319 observations deleted due to missingness)\nAIC: 15580\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  24.75 \n          Std. Err.:  1.49 \nWarning while fitting theta: alternation limit reached \n\n 2 x log-likelihood:  -15541.83 \n\n\n\ncrime_summary_race_model &lt;- crime_summary_race %&gt;%\n  filter(complete.cases(\n    avg_median_income, Hispanic.Latin.Mexican, White, Black, Other, Other.Asian,\n    Unknown, Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian,\n    American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian\n  ))\n\n# Add predicted values to the filtered data\ncrime_summary_race_model &lt;- crime_summary_race_model %&gt;%\n  mutate(predicted_crimes = predict(bivar_nb_model, type = \"response\"))\n\ncrime_race_long &lt;- crime_summary_race_model %&gt;%\n  pivot_longer(\n    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, \n             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, \n             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),\n    names_to = \"race\",\n    values_to = \"crime_count\"\n  )\n\n\ncrime_race_long &lt;- crime_summary_race %&gt;%\n  pivot_longer(\n    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, \n             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, \n             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),\n    names_to = \"race\",\n    values_to = \"crime_count\"\n  ) %&gt;%\n  filter(!is.na(crime_count) & crime_count &gt; 0) %&gt;%\n  mutate(avg_median_income = avg_median_income / 1000)\n\n\n\n# Fit Poisson regression with pivoted data\npoisson_model &lt;- glm(\n  crime_count ~ avg_median_income + race,\n  family = poisson(link = \"log\"),\n  data = crime_race_long\n)\nsummary(poisson_model)\n\n\nCall:\nglm(formula = crime_count ~ avg_median_income + race, family = poisson(link = \"log\"), \n    data = crime_race_long)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 0.1632419  0.2298224   0.710 0.477521    \navg_median_income          -0.0019821  0.0002549  -7.777 7.42e-15 ***\nraceAsian.Indian            0.0155105  0.4422303   0.035 0.972021    \nraceBlack                   0.6206238  0.2308355   2.689 0.007175 ** \nraceCambodian               0.1967529  0.5026375   0.391 0.695471    \nraceChinese                 0.1221015  0.2668446   0.458 0.647257    \nraceFilipino               -0.0125247  0.2612828  -0.048 0.961768    \nraceHawaiian               -0.0256375  0.4683073  -0.055 0.956342    \nraceHispanic.Latin.Mexican  0.8102344  0.2300788   3.522 0.000429 ***\nraceJapanese                0.0635533  0.3100358   0.205 0.837582    \nraceKorean                  0.1226668  0.2558546   0.479 0.631626    \nraceLaotian                -0.0216528  1.0259884  -0.021 0.983162    \nraceOther                   0.3349309  0.2320411   1.443 0.148905    \nraceOther.Asian             0.1312525  0.2378787   0.552 0.581111    \nraceUnknown                 0.7716858  0.2301287   3.353 0.000799 ***\nraceVietnamese             -0.0043229  0.3071443  -0.014 0.988771    \nraceWhite                   0.6653885  0.2304941   2.887 0.003892 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6300.2  on 5907  degrees of freedom\nResidual deviance: 5790.0  on 5891  degrees of freedom\n  (339 observations deleted due to missingness)\nAIC: 20106\n\nNumber of Fisher Scoring iterations: 5\n\n# Fit Negative Binomial regression with pivoted data\ncrime_race_long &lt;- crime_race_long %&gt;%\n  filter(complete.cases(avg_median_income, race, crime_count))\n\nnb_model &lt;- glm.nb(\n  crime_count ~ avg_median_income * race,\n  data = crime_race_long\n)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = crime_count ~ avg_median_income * race, data = crime_race_long, \n    init.theta = 9.252493725, link = log)\n\nCoefficients: (1 not defined because of singularities)\n                                               Estimate Std. Error z value\n(Intercept)                                   0.1455120  0.4202718   0.346\navg_median_income                            -0.0016531  0.0063198  -0.262\nraceAsian.Indian                             -0.1455120  1.0570790  -0.138\nraceBlack                                     0.7861658  0.4246697   1.851\nraceCambodian                                 0.4754042  1.0460402   0.454\nraceChinese                                  -0.1475022  0.5367116  -0.275\nraceFilipino                                 -0.1663137  0.5407560  -0.308\nraceHawaiian                                 -0.1455120  1.0060103  -0.145\nraceHispanic.Latin.Mexican                    1.0450224  0.4226039   2.473\nraceJapanese                                  0.1871495  0.6428169   0.291\nraceKorean                                    0.1289437  0.4757716   0.271\nraceLaotian                                  -0.0274248  1.0855404  -0.025\nraceOther                                     0.2422653  0.4272637   0.567\nraceOther.Asian                               0.0666385  0.4443021   0.150\nraceUnknown                                   0.7903132  0.4223953   1.871\nraceVietnamese                               -0.1455120  0.6123748  -0.238\nraceWhite                                     0.4589181  0.4231629   1.084\navg_median_income:raceAsian.Indian            0.0016531  0.0115164   0.144\navg_median_income:raceBlack                  -0.0027989  0.0063851  -0.438\navg_median_income:raceCambodian              -0.0043472  0.0145757  -0.298\navg_median_income:raceChinese                 0.0030248  0.0071816   0.421\navg_median_income:raceFilipino                0.0021817  0.0077737   0.281\navg_median_income:raceHawaiian                0.0016531  0.0130268   0.127\navg_median_income:raceHispanic.Latin.Mexican -0.0038425  0.0063534  -0.605\navg_median_income:raceJapanese               -0.0019210  0.0091102  -0.211\navg_median_income:raceKorean                 -0.0001553  0.0069246  -0.022\navg_median_income:raceLaotian                        NA         NA      NA\navg_median_income:raceOther                   0.0011020  0.0063781   0.173\navg_median_income:raceOther.Asian             0.0008324  0.0065657   0.127\navg_median_income:raceUnknown                -0.0003425  0.0063433  -0.054\navg_median_income:raceVietnamese              0.0016531  0.0079050   0.209\navg_median_income:raceWhite                   0.0023250  0.0063396   0.367\n                                             Pr(&gt;|z|)  \n(Intercept)                                    0.7292  \navg_median_income                              0.7937  \nraceAsian.Indian                               0.8905  \nraceBlack                                      0.0641 .\nraceCambodian                                  0.6495  \nraceChinese                                    0.7835  \nraceFilipino                                   0.7584  \nraceHawaiian                                   0.8850  \nraceHispanic.Latin.Mexican                     0.0134 *\nraceJapanese                                   0.7709  \nraceKorean                                     0.7864  \nraceLaotian                                    0.9798  \nraceOther                                      0.5707  \nraceOther.Asian                                0.8808  \nraceUnknown                                    0.0613 .\nraceVietnamese                                 0.8122  \nraceWhite                                      0.2781  \navg_median_income:raceAsian.Indian             0.8859  \navg_median_income:raceBlack                    0.6611  \navg_median_income:raceCambodian                0.7655  \navg_median_income:raceChinese                  0.6736  \navg_median_income:raceFilipino                 0.7790  \navg_median_income:raceHawaiian                 0.8990  \navg_median_income:raceHispanic.Latin.Mexican   0.5453  \navg_median_income:raceJapanese                 0.8330  \navg_median_income:raceKorean                   0.9821  \navg_median_income:raceLaotian                      NA  \navg_median_income:raceOther                    0.8628  \navg_median_income:raceOther.Asian              0.8991  \navg_median_income:raceUnknown                  0.9569  \navg_median_income:raceVietnamese               0.8344  \navg_median_income:raceWhite                    0.7138  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(9.2525) family taken to be 1)\n\n    Null deviance: 4800.1  on 5907  degrees of freedom\nResidual deviance: 4305.8  on 5877  degrees of freedom\nAIC: 19758\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  9.252 \n          Std. Err.:  0.706 \n\n 2 x log-likelihood:  -19694.184 \n\ncrime_race_long &lt;- crime_race_long %&gt;%\n  mutate(predicted_crimes = predict(nb_model, newdata = crime_race_long, type = \"response\"))\n\n\ncolnames(crime_race_long)\n\n[1] \"GEOID\"             \"avg_median_income\" \"geometry\"         \n[4] \"total_crimes\"      \"race\"              \"crime_count\"      \n[7] \"predicted_crimes\" \n\ngrid &lt;- expand.grid(\n  avg_median_income = seq(min(crime_race_long$avg_median_income), max(crime_race_long$avg_median_income), length.out = 100),\n  race = unique(crime_race_long$race)\n)\n\ngrid$predicted_crimes &lt;- predict(nb_model, newdata = grid, type = \"response\")\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nggplot(grid, aes(x = avg_median_income, y = predicted_crimes, color = race)) +\n  geom_line() +\n  labs(\n    title = \"Predicted Crime Counts vs Median Income by Race\",\n    x = \"Average Median Income (in $1,000s)\",\n    y = \"Predicted Crime Counts\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(grid, aes(x = avg_median_income, y = predicted_crimes, color = race)) +\n  geom_line() +\n  facet_wrap(~race, scales = \"free_y\") +  # Facet by race, allowing y-axis to vary by panel\n  labs(\n    title = \"Predicted Crime Counts vs Median Income by Race (Facet by Race)\",\n    x = \"Average Median Income (in $1,000s)\",\n    y = \"Predicted Crime Counts\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),  # Adjust facet labels\n    legend.position = \"none\"              # Hide legend if redundant\n  )\n\n\n\n\n\n\n\n\n\ncrime_summary_blocks &lt;- crime_within_lapd %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarize(\n    total_crimes = n(),\n    geometry = st_union(geometry)  # Combine geometries for each group\n  ) %&gt;%\n  filter(!is.na(total_crimes)) %&gt;%  # Exclude areas with no crimes\n  st_as_sf()  # Ensure it remains an sf object\n\nprint(crime_summary_blocks)\n\nSimple feature collection with 21 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -118.6676 ymin: 33.7087 xmax: -118.1585 ymax: 34.3272\nGeodetic CRS:  WGS 84\n# A tibble: 21 × 3\n   AREA.NAME   total_crimes                                             geometry\n * &lt;chr&gt;              &lt;int&gt;                                     &lt;MULTIPOINT [°]&gt;\n 1 77th Street          823 ((-118.3113 33.94), (-118.3123 33.9418), (-118.3113…\n 2 Central              891 ((-118.2292 34.0365), (-118.2294 34.0385), (-118.23…\n 3 Devonshire           546 ((-118.5981 34.2713), (-118.5916 34.2691), (-118.58…\n 4 Foothill             431 ((-118.4389 34.2644), (-118.4385 34.2621), (-118.43…\n 5 Harbor               532 ((-118.3041 33.8481), (-118.3035 33.855), (-118.303…\n 6 Hollenbeck           428 ((-118.1861 34.1077), (-118.1987 34.0915), (-118.20…\n 7 Hollywood            636 ((-118.304 34.0835), (-118.3028 34.0836), (-118.300…\n 8 Mission              511 ((-118.4926 34.3197), (-118.4958 34.3216), (-118.40…\n 9 N Hollywood          662 ((-118.3878 34.2067), (-118.3891 34.2067), (-118.39…\n10 Newton               584 ((-118.23 34.0326), (-118.2322 34.0306), (-118.2323…\n# ℹ 11 more rows\n\ncrime_summary_blocks &lt;- st_set_crs(crime_summary_blocks, 4326)\n\n\n# Load LAPD boundary\nlapd_boundary &lt;- st_read(here(\"CityBoundaryofLosAngeles/geo_export_416eaeda-447a-4473-b003-37e2cad181ac.shp\")) %&gt;%\n  st_make_valid() %&gt;%                # Ensure valid geometry\n  st_transform(crs = 4326)           # Align CRS (use WGS84 as default)\n\nReading layer `geo_export_416eaeda-447a-4473-b003-37e2cad181ac' from data source `/Users/oscarmo/ma415/ma-4615-fa24-final-project-group-4/CityBoundaryofLosAngeles/geo_export_416eaeda-447a-4473-b003-37e2cad181ac.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -118.6682 ymin: 33.70365 xmax: -118.1554 ymax: 34.33731\nGeodetic CRS:  WGS84(DD)\n\n# Load neighborhoods\nneighborhoods &lt;- st_read(here(\"LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp\")) %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = st_crs(lapd_boundary))  # Match CRS with LAPD boundary\n\nReading layer `8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu' from data source `/Users/oscarmo/ma415/ma-4615-fa24-final-project-group-4/LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 6359592 ymin: 1715035 xmax: 6514633 ymax: 1945515\nProjected CRS: NAD83 / California zone 5 (ftUS)\n\nneighborhoods_lapd &lt;- st_intersection(neighborhoods, lapd_boundary)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nneighborhoods_lapd &lt;- st_buffer(neighborhoods, dist = 0) %&gt;%\n  st_intersection(st_buffer(lapd_boundary, dist = 0))\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\ncrime_sf &lt;- st_as_sf(data, coords = c(\"LON\", \"LAT\"), crs = 4326)  # Crime data as sf object\n\n# Filter crimes to those within LAPD boundary\ncrimes_in_lapd &lt;- st_filter(crime_sf, lapd_boundary)\n\nneighborhoods_lapd &lt;- st_make_valid(neighborhoods_lapd)\ncrimes_in_lapd &lt;- st_make_valid(crimes_in_lapd)\n\n# Join crimes to neighborhoods\ncrimes_by_neighborhood &lt;- st_join(crimes_in_lapd, neighborhoods_lapd, join = st_within)\n\n\n# Ensure crimes_by_neighborhood includes counts and neighborhood polygons\ncrime_summary &lt;- neighborhoods_lapd %&gt;%\n  left_join(\n    crimes_by_neighborhood %&gt;%\n      st_drop_geometry() %&gt;%  # Drop point geometry to focus on attributes\n      group_by(name) %&gt;%      # Replace \"NAME\" with neighborhood column in neighborhoods_lapd\n      summarize(total_crimes = n()),\n    by = \"name\"               # Join by neighborhood name or another matching column\n  )\n\n\n\nggplot() +\n  geom_sf(data = neighborhoods_lapd, fill = NA, color = \"black\") +  # Neighborhood outlines\n  geom_sf(data = crime_summary, aes(fill = total_crimes), color = NA) +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"grey50\") +\n  labs(\n    title = \"Heatmap of Crimes by Neighborhood (LAPD Jurisdiction)\",\n    fill = \"Total Crimes\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 12)\n  )"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-10-31-blog-post-4/blog-post-4.html",
    "href": "posts/2024-10-31-blog-post-4/blog-post-4.html",
    "title": "blog post 4",
    "section": "",
    "text": "library(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA 415 G4 Final Project\n\ndata &lt;- readRDS(here(\"filtered_data_2.rds\"))\n\nfiltered_data &lt;- subset(data, Descent_Description != \"Unknown\" & Descent_Description != \"Other\")\n\n# Create a contingency table for the filtered data\ncrime_table_filtered &lt;- table(filtered_data$Descent_Description, filtered_data$Crm.Cd.Desc)\n\nchi_test_filtered &lt;- chisq.test(crime_table_filtered)\n\nWarning in chisq.test(crime_table_filtered): Chi-squared approximation may be\nincorrect\n\nprint(chi_test_filtered)\n\n\n    Pearson's Chi-squared test\n\ndata:  crime_table_filtered\nX-squared = 2363.8, df = 1222, p-value &lt; 2.2e-16\n\n# Extract standardized residuals\nstandardized_residuals_filtered &lt;- chi_test_filtered$stdres\n\ntop_crimes &lt;- names(sort(colSums(crime_table_filtered), decreasing = TRUE))[1:20]\n\n# Filter the contingency table to only include top crime types\ncrime_table_top &lt;- crime_table_filtered[, top_crimes]\n\nchi_test_top &lt;- chisq.test(crime_table_top)\n\nWarning in chisq.test(crime_table_top): Chi-squared approximation may be\nincorrect\n\nstandardized_residuals_top &lt;- chi_test_top$stdres\n\n\nresiduals_top_df &lt;- as.data.frame(as.table(standardized_residuals_top))\ncolnames(residuals_top_df) &lt;- c(\"Descent\", \"Crime_Type\", \"Residual\")\n\n\n\nggplot(residuals_top_df, aes(x = Crime_Type, y = Descent, fill = Residual)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  labs(title = \"Standardized Residuals of Descent and Top Crime Types\",\n       x = \"Crime Type\", y = \"Descent Category\", fill = \"Residual\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nI wanted to ask the question of whether every crime was correlated to descent. Therefore, I conducted a chi-squared test and wanted to see how each race’s residual was in comparison to the top 10 crimes (all crimes would be too much). - We can see that “White” had a high residual in the burglary category and higher residuals in some other categories as well. We also see lower than average residuals in the aggrevated assault and simple assault categories. - As for Hispanic/Mexican, even though they represent such a high amount of crimes (as seen in last blog post), they are moderately around what is expected with the exception of the “Burglary” and “Theft (under $950) categories. - Other Asian slightly higher in”Burglary” - “Black” lower in “theft from motor vehicle”. - “Filipino” higher in “theft from motor vehicle”. - “Chinese” higher in “theft - grand except guns or livestock”. - Everything else is mostly the same hovering around the expected with small fluctuations.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncrime_hour_descent &lt;- data %&gt;%\n  group_by(Hour, Crm.Cd.Desc, Descent_Description) %&gt;%\n  summarise(Count = n(), .groups = \"drop\") %&gt;%\n  filter(Count &gt; 0)\n\nggplot(crime_hour_descent, aes(x = Hour, y = Descent_Description, fill = Count)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkred\", name = \"Crime Count\") + \n  labs(title = \"Heatmap of Crime Counts by Hour and Descent\",\n       x = \"Hour of Day\",\n       y = \"Descent Category\")\n\n\n\n\n\n\n\n\nI make this plot to visualize how crime frequency varies for each descent category by time of the day. I use heatmap since it provides a direct comparison across different times and descents. This plot clearly shows that certain racial categories, such as White, Hispanic/Latin/Mexican, and Black, experience higher crime rates compared to racial groups like Cambodian and Laotian, potentially due to smaller sample sizes or data collection biases.\nThe general trend across most categories indicates a peak in crime rates from late evening to early morning (00:00 to 05:00), which is the same as what we expected. However, an additional peak occurs during daytime (10:00 to 15:00), which is unusual and needs further investigation.\n\nlibrary(dplyr)\nlibrary(here)\ncrime_data &lt;- readRDS(here(\"filtered_data_2.rds\")) %&gt;%\n  mutate(across(c(AREA.NAME, Crm.Cd.Desc, Descent_Description), ~ trimws(tolower(.))))\ncrime_area_descent_summary &lt;- crime_data %&gt;%\n  count(AREA.NAME, Crm.Cd.Desc, Descent_Description, name = \"Crime_Count\")\n\n# Get top 10 areas and crimes\ntop_10_areas &lt;- crime_area_descent_summary %&gt;%\n  count(AREA.NAME, wt = Crime_Count, sort = TRUE) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(AREA.NAME)\ntop_10_crimes &lt;- crime_area_descent_summary %&gt;%\n  count(Crm.Cd.Desc, wt = Crime_Count, sort = TRUE) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(Crm.Cd.Desc)\n\nfiltered_data &lt;- crime_area_descent_summary %&gt;%\n  filter(AREA.NAME %in% top_10_areas, Crm.Cd.Desc %in% top_10_crimes)\n\n# Chi-square test for Crime Type vs. Area\ncrime_type_area&lt;- xtabs(Crime_Count ~ Crm.Cd.Desc + AREA.NAME, data = filtered_data)\nchi_test_crime_area &lt;- chisq.test(crime_type_area)\nprint(\"Chi-Square Test Result: Crime Type vs. Area\")\n\n[1] \"Chi-Square Test Result: Crime Type vs. Area\"\n\nprint(chi_test_crime_area)\n\n\n    Pearson's Chi-squared test\n\ndata:  crime_type_area\nX-squared = 451.13, df = 81, p-value &lt; 2.2e-16\n\n# Chi-square test for Descent Group vs. Area\ndescent_area &lt;- xtabs(Crime_Count ~ Descent_Description + AREA.NAME, data = filtered_data)\nchi_test_descent_area &lt;- chisq.test(descent_area)\n\nWarning in chisq.test(descent_area): Chi-squared approximation may be incorrect\n\nprint(\"Chi-Square Test Result: Descent Group vs. Area\")\n\n[1] \"Chi-Square Test Result: Descent Group vs. Area\"\n\nprint(chi_test_descent_area)\n\n\n    Pearson's Chi-squared test\n\ndata:  descent_area\nX-squared = 1051.2, df = 135, p-value &lt; 2.2e-16\n\n\nboth of the chi-square tests have low p-value, indicating that the variables are statistically associated. With the chi-sqaure test on crime type - descent group, I wanted to understand the distribution across these three variables at once.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Plot\nggplot(filtered_data, aes(x = reorder(Crm.Cd.Desc, -Crime_Count), y = Crime_Count, fill = AREA.NAME)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Descent_Description, ncol = 3) +\n  labs(\n    title = \"Most Prevalent Area for Top Crimes by Descent Group (Top 10 Areas and Crimes)\",\n    x = \"Crime Type\",\n    y = \"Crime Count\",\n    fill = \"Area\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 6),\n    strip.text = element_text(size = 10),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nThis graph shows the distributions across crime types, geographic areas, and descent groups within the top 10 most frequent crime types and top 10 areas(it’s actually 11 areas in the graph: ‘hollywood’ and ’ n hollywood’ are counted as separate entries) with the highest crime counts. Here are some key patterns from the graph: noted that some descent groups, like Black, Hispanic/Latin/Mexican, and White, experienced a wider range of crime types across multiple areas, whereas others had more limited exposure. Certain areas, such as 77th Street, Central, and Southwest, consistently showed high counts across multiple crime types, showing that they are hopspts for various types of crime. Last but not least, vehicle theft, simple assault, and petty theft were widespread acroos areas and descent groups."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\nSean Fung, Zihao Guo, Siqi Chen\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 7\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 7\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 6\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nSean Fung, Zihao Guo, Yawen Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 5\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nSean Fung, Zihao Guo, Yawen Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nblog post 4\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post 3\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nSean Fung\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#relationship-between-crime-and-median-income",
    "href": "analysis.html#relationship-between-crime-and-median-income",
    "title": "Analysis",
    "section": "Relationship between crime and median income",
    "text": "Relationship between crime and median income\n\n\n\n\n\n\n\n\n\nGraph explanation: The color from dark to bright means median income from low to high.\nThe graph shows that there is no clear relationship between median income and total crimes based on the data points presented. While there is a general trend of lower median incomes associated with higher total crimes, there are several outliers that do not fit this pattern. Some areas with relatively high median incomes also have high total crime rates, and vice versa. This suggests that factors beyond just income level likely play a significant role in determining crime rates within a community. Simply assuming that higher-income neighborhoods will have lower crime rates oversimplifies the complex social, economic, and environmental factors that contribute to public safety. A more nuanced analysis would be needed to fully understand the relationship between these variables across different geographic areas. Our team decide to investigate the frequent of property crime by the each neighborhood that sort by their median income."
  },
  {
    "objectID": "analysis.html#violent-crime-in-each-neighborhood-order-by-their-income",
    "href": "analysis.html#violent-crime-in-each-neighborhood-order-by-their-income",
    "title": "Analysis",
    "section": "Violent crime in each neighborhood order by their income",
    "text": "Violent crime in each neighborhood order by their income\n\n\n\n\n\n\n\n\n\nGraph explanation: The x-axis(areas) are order in ascending median income order, which we will expected from left to right neighborhood, the count of property crime will increase.\nThe chart reveals a notable outlier: the Central neighborhood, which reports a disproportionately high number of violent crimes relative to its median income. While the general trend aligns with the assumption—often rooted in societal stereotypes—that lower-income neighborhoods experience higher levels of violent crime, Central deviates from this expectation. Despite not having the lowest median income, its violent crime rate far exceeds those of other neighborhoods. This suggests that relying solely on income as a predictor of crime may oversimplify a more complex issue. Factors such as geographic location, population density, proximity to transit hubs, or the concentration of commercial and nightlife activities might better explain the elevated crime rate in Central. This highlights the importance of challenging stereotypes and exploring a broader range of influences when analyzing crime patterns."
  },
  {
    "objectID": "analysis.html#crime-count-distribution-in-heatmap",
    "href": "analysis.html#crime-count-distribution-in-heatmap",
    "title": "Analysis",
    "section": "Crime count distribution in heatmap",
    "text": "Crime count distribution in heatmap\n\n\n\n\n\n\n\n\n\nThis heatmap visualizes the spatial distribution of total crimes across neighborhoods under the LAPD jurisdiction, with higher crime rates indicated by brighter yellow hues and lower crime rates shown in darker purple. The outlier observed in the previous bar chart, Central, is corroborated here as one of the brightest regions on the map, indicating a notably high concentration of crimes.\nWhen connecting this to the previous graph, we see that Central’s geographic location likely plays a significant role in its elevated crime levels. The neighborhood’s centrality and potential proximity to high-density areas, transit hubs, or commercial districts could contribute to the high crime numbers. This geographic clustering of crime challenges the stereotype that violent crime is solely tied to lower-income areas, as Central’s median income is not the lowest yet its crime rate is disproportionately high. It suggests that spatial dynamics, such as the presence of key urban features and high activity zones, may compound the crime rate independent of income alone.\n\nincome_inequality &lt;- crime_with_income %&gt;%\n  group_by(AREA.NAME) %&gt;%\n  summarize(\n    income_range = max(estimate, na.rm = TRUE) - min(estimate, na.rm = TRUE),\n    total_crimes = n()\n  )\nggplot(income_inequality, aes(x = income_range, y = total_crimes)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Income Inequality vs. Total Crimes\",\n    x = \"Income Range (Proxy for Inequality)\",\n    y = \"Total Crimes\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn our earlier analysis, we observed that economic factors, such as median income, play a role in shaping crime dynamics. Here, we extend our exploration to examine whther socioeconomic inequality within neighborhoods might also contribute to crime disparities. The figure above examines the relationship between income inequality and the total number of crimes in neighborhoods. The red line shows a slight negative slope, suggesting that as income inequality increases, total crimes tend to decrease marginally. The data points are widely scattered around the line, reflecting significant variability in total crimes across neighborhoods with similar income ranges. This implies that factors beyond income inequality likely influence crime rates."
  },
  {
    "objectID": "analysis.html#motivation-for-the-model",
    "href": "analysis.html#motivation-for-the-model",
    "title": "Analysis",
    "section": "Motivation for the Model",
    "text": "Motivation for the Model\nWe aim to understand the relationship between median income and crime, considering racial demographics as a moderating factor. This requires modeling total crimes as a function of average median income and racial groupings. To account for overdispersion in crime count data, we employ both Poisson regression and Negative Binomial regression frameworks."
  },
  {
    "objectID": "analysis.html#model-selection",
    "href": "analysis.html#model-selection",
    "title": "Analysis",
    "section": "Model Selection",
    "text": "Model Selection\nWhy Poisson and Negative Binomial Models?\nCrime counts are discrete and non-negative, making Poisson regression an intuitive starting point.\nOverdispersion (variance exceeding the mean) was evident, prompting the use of a Negative Binomial regression model. Predictors Considered\nMedian income: Hypothesized to influence crime rates inversely.\nRace: Used as categorical predictors to examine demographic disparities in crime incidence.\nInteraction terms: Incorporated to explore how income effects vary across racial groups."
  },
  {
    "objectID": "analysis.html#modeling-steps",
    "href": "analysis.html#modeling-steps",
    "title": "Analysis",
    "section": "Modeling Steps",
    "text": "Modeling Steps\n\nPoisson regression model:\n\nCoefficients:\n\navg_median_income: -0.0019821 (p &lt; 0.001, significant)\nraceBlack: 0.6206 (p = 0.007, significant)\nraceHispanic.Latin.Mexican: 0.8102 (p &lt; 0.001, significant)\nraceUnknown: 0.7717 (p &lt; 0.001, significant)\nraceWhite: 0.6654 (p = 0.004, significant)\n\n\n\nModel Metrics:\n\nNull deviance: 6300.2 on 5907 degrees of freedom\nResidual deviance: 5790.0 on 5891 degrees of freedom\nAIC: 20106\n\nKey Insights:\navg_median_income is negatively associated with crime counts and is statistically significant.\nCertain racial categories (raceHispanic.Latin.Mexican, raceUnknown, raceWhite, and raceBlack) are positively associated with higher crime counts and are statistically significant.\n\n\n\n\nNegative Binomial regression model:\n\nCoefficients:\n\nraceHispanic.Latin.Mexican: 1.0450 (p = 0.013, significant)\nraceUnknown: 0.7903 (p = 0.061, marginally significant)\nraceBlack: 0.7862 (p = 0.064, marginally significant)\n\n\n\nModel Metrics:\n\nNull deviance: 4800.1 on 5907 degrees of freedom\nResidual deviance: 4305.8 on 5877 degrees of freedom\nAIC: 19758\nDispersion parameter: Theta = 9.252 (SE = 0.706)\n\nKey Insights:\nraceHispanic.Latin.Mexican remains statistically significant and positively associated with crime counts.\nThe inclusion of overdispersion (via the Negative Binomial model) leads to better model fit metrics (lower AIC and deviance) compared to the Poisson regression.\nWe evaluate model fit using AIC and residual diagnostics, concluding the Negative Binomial model better accounts for data characteristics.\n\n\n\n\nWhy Negative Binomial Was Better\nDispersion Issue: The Poisson model assumes that the mean and variance of the dependent variable (crime counts) are equal. However, the data exhibits overdispersion (variance &gt; mean), as indicated by the high residual deviance in the Poisson model.\nModel Fit: The Negative Binomial regression explicitly accounts for overdispersion with a dispersion parameter (Theta = 9.252), leading to improved model fit (lower AIC: 19758 vs. 20106 and reduced deviance).\nInterpretation: While both models identify significant predictors (e.g., raceHispanic.Latin.Mexican), the Negative Binomial model provides more reliable estimates due to its ability to handle overdispersed data."
  },
  {
    "objectID": "analysis.html#predicted-trends",
    "href": "analysis.html#predicted-trends",
    "title": "Analysis",
    "section": "Predicted Trends",
    "text": "Predicted Trends\nUsing the model, we predict crime counts across income ranges for each racial group:\n\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nThis figure illustrates how income impacts predicted crime counts, highlighting disparities among racial groups.\nThis also removed any race with a P-Value over 0.8 and any races like “unknown” and “other” because that doesn’t give much information. We can see that for the most part, as median income increases, the likelihood of a crime decreases. This is not the case for White, Chinese, and Filipino which might be worth looking into.\n\n\nUncertainty and Limitations\n1. Uncertainty in Estimates:\n\nWide confidence intervals for certain racial group coefficients suggest variability in observed patterns.\n\nLimited data for smaller racial groups (e.g., Laotian, Cambodian) may result in unreliable estimates.\n\n\n2.Model Assumptions:\n\nBoth Poisson and Negative Binomial models assume log-linear relationships, which may oversimplify real-world dynamics.\n\nSpatial dependencies (e.g., crime clustering in neighborhoods) are not accounted for, potentially biasing results.\n\n\n3. Data Limitations:\n\nThe dataset has missing values for some predictors, leading to excluded observations.\n\nAggregation by neighborhood may obscure within-neighborhood variability in income and crime rates.\n\n\n\n\nFuture Directions\n\nIntegrate additional predictors like unemployment rates, educational attainment, or policing density to refine insights.\n\nPerform robustness checks by using alternative categorizations for racial groups or income brackets."
  },
  {
    "objectID": "shiny_dataset/shiny_data.html",
    "href": "shiny_dataset/shiny_data.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidycensus)\nlibrary(here)\n\nhere() starts at /Users/seanfung/Documents/BU Fall 2024/MA 415/MA415 G4 Final Project\n\ndata &lt;- readRDS(here(\"filtered_data_2.rds\"))\ncrime_sf &lt;- st_as_sf(data, coords = c(\"LON\", \"LAT\"), crs = 4326)\n\n# Load LAPD boundary shapefile\nlapd_boundary &lt;- st_read(here(\"CityBoundaryofLosAngeles/geo_export_416eaeda-447a-4473-b003-37e2cad181ac.shp\")) %&gt;%\n  st_transform(st_crs(crime_sf))\n\nReading layer `geo_export_416eaeda-447a-4473-b003-37e2cad181ac' from data source `/Users/seanfung/Documents/BU Fall 2024/MA 415/MA415 G4 Final Project/CityBoundaryofLosAngeles/geo_export_416eaeda-447a-4473-b003-37e2cad181ac.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -118.6682 ymin: 33.70365 xmax: -118.1554 ymax: 34.33731\nGeodetic CRS:  WGS84(DD)\n\n# Filter crime data within LAPD jurisdiction\ncrime_within_lapd &lt;- crime_sf %&gt;%\n  filter(st_within(geometry, lapd_boundary, sparse = FALSE))\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\n\n# Set up Census API key\ncensus_api_key(\"c2aebe6041f0c99e41a3458ed8d0b95ee3650fa4\")\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n# Load and transform economic data\neconomic_data &lt;- get_acs(\n  geography = \"block group\",\n  variables = c(median_income = \"B19013_001\"),\n  state = \"CA\",\n  county = \"Los Angeles\",\n  year = 2020,\n  geometry = TRUE\n) %&gt;%\n  st_transform(st_crs(crime_within_lapd))\n\nGetting data from the 2016-2020 5-year ACS\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Filter economic data within LAPD boundary\neconomic_data_lapd &lt;- st_intersection(economic_data, lapd_boundary)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# Perform spatial join to combine crime and economic data\ncrime_with_income_lapd &lt;- st_join(crime_within_lapd, economic_data_lapd, join = st_within)\n\n# Save the processed dataset\nsaveRDS(crime_with_income_lapd, file = here(\"crime_with_income_lapd.rds\"))\nsaveRDS(economic_data_lapd, file= here(\"economic_data_lapd.rds\"))"
  }
]