---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)


This comes from the file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.


Initially, you will study _one dataset_ but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable.
Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.


## What makes a good data set?

* Data you are interested in and care about.
* Data where there are a lot of potential questions that you can explore.
* A data set that isn't completely cleaned already.
* Multiple sources for data that you can combine.
* Some type of time and/or location component.


## Where to keep data?


Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data


For small datasets (<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  "scripts/load_and_clean_data.R",
  echo = TRUE # Use echo=FALSE or omit it to avoid code output  
)
```
You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R). 

----

## Rubric: On this page

You will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Rrename variables and recode factors to make data more clear.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.
  

Analysis and Cleaning of LAPD Crime Data (2020â€“Present)

## 1. Finding the Data
The dataset we use reflects incidents of crime in Los Angeles reported by the LAPD since 2020. The original data source is:
  * Source: https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8/data_preview
  * Attribution: This data is transcribed from original crime reports that are typed on paper.
  * Purpose of Collection: The LAPD collects this data as part of routine public safety operations in order to better track crime patterns, allocating resources, and for transparency purposes, ensuring that the public has access to crime data. By making this dataset publicly accessible, the LAPD ensures that the public has access to crime data, fostering accountability in law enforcement operations.
  * Potential Challenges: Handling a large volume of data could present computational challenges, especially if we plan to conduct detailed spatial or temporal analyses. Cleaning might involve filtering duplicates or dealing with incomplete reports.

## 2. Variable Interpretations
  * There are 5 types of files of this dataset that were provided on the link we mentioned above, they are Comma Separated Values(CSV) File, Tab Separated Values(TSV) File, RDF File, JSON File, and XML File. We chose to download the CSV file.
  * There are more than 987K rows and 28 columns for the original dataset, which means that there are 28 different variables, they are DR_NO, Date Rptd, DATE OCC, TIME OCC, AREA, AREA NAME, Rpt Dist No, Part 1-2, Crm Cd, Crm Cd Desc, Mocodes, Vict Age, Vict Sex, Vict Descent, Premis Cd, Premis Desc, Weapon Used Cd, Weapon Desc, Status, Status Desc, Crm Cd 1, Crm Cd 2, Crm Cd 3, Crm Cd 4, LOCATION, Cross Street, LAT, and LON.
  * Here's the description and datatype of these variables:
    * DR_NO(Text): Division of Records Number: Official file number made up of a 2 digit year, area ID, and 5 digits.
    * Date Rptd(Floating Timestamp): The date the crime was reported to the LAPD, formatted as MM/DD/YYYY.
    * DATE OCC(Floating Timestamp): The date the crime occurred, formatted as MM/DD/YYYY.
    * TIME OCC(Text): The time the crime occurred, recorded in 24-hour military time.
    * AREA(Text): The LAPD has 21 Community Police Stations referred to as Geographic Areas within the department. These Geographic Areas are sequentially numbered from 1-21.
    * AREA NAME(Text): The 21 Geographic Areas or Patrol Divisions are also given a name designation that references a landmark or the surrounding community that it is responsible for. For example 77th Street Division is located at the intersection of South Broadway and 77th Street, serving neighborhoods in South Los Angeles.
    * Rpt Dist No(Text): A four-digit code that represents a sub-area within a Geographic Area. All crime records reference the "RD" that it occurred in for statistical comparisons.
    * Part 1-2(Number): Classification of the crime as Part I or Part II offense.
    * Crm Cd(Text): Indicates the crime committed. (Same as Crime Code 1)
    * Crm Cd Desc(Text): Defines the Crime Code provided.
    * Mocodes(Text): Activities associated with the suspect in commission of the crime.
    * Vict Age(Text): Age of the victim involved in the incident.
    * Vict Sex(Text): Gender of the victim (M for Male, F for Female, X for Unknown).
    * Vict Descent(Text): Ethnic descent of the victim, coded (e.g., H for Hispanic, W for White).
    * Premis Cd(Number): The type of structure, vehicle, or location where the crime took place.
    * Premis Desc(Text): Defines the Premise Code provided.
    * Weapon Used Cd(Text): The type of weapon used in the crime.
    * Weapon Desc(Text): Defines the Weapon Used Code provided.
    * Status(Text): Status of the case. (IC is the default)
    * Status Desc(Text): Defines the Status Code provided.
    * Crm Cd 1(Text): Indicates the crime committed. Crime Code 1 is the primary and most serious one.
    * Crm Cd 2(Text): May contain a code for an additional crime, less serious than Crime Code 1.
    * Crm Cd 3(Text): May contain a code for an additional crime, less serious than Crime Code 1.
    * Crm Cd 4(Text): May contain a code for an additional crime, less serious than Crime Code 1.
    * LOCATION(Text): Street address of crime incident rounded to the nearest hundred block to maintain anonymity.
    * Cross Street(Text): Cross Street of rounded Address.
    * LAT(Number): Latitude
    * LON(Number): Longtitude

## 3. Data Cleaning
Here is the original Dataset that we have:
```{r}
library(here)
library(readr)
data <- read_csv(here("your_file.csv"))
```

Then we start the data cleaning process. 

The initial data cleaning focused on filtering and sampling the dataset to ensure manageable and relevant data for analysis. After loading the raw data, the DATE.OCC column, which represents the date the crime occurred, was converted into a numerical year column. This transformation allowed us to filter data for the specific range of years between 2020 and 2024. The filtered dataset was further refined by grouping it by year and sampling up to 2,500 rows per year to balance representation while ensuring computational efficiency. 

The following r chunk is the exact code we use by following the previous steps.
```{r}
# Convert the date column to year format
data$year <- format(as.Date(data$DATE.OCC, format = "%m/%d/%Y"), "%Y")
# Convert year to numeric
data$year <- as.numeric(data$year)  
# Filter for the years 2020â€“2024
filtered_years <- data %>%
  filter(year %in% 2020:2024)
# Split by year and sample up to 2,500 rows per year
sampled_data <- filtered_years %>%
  group_split(year) %>%
  map_df(~ slice_sample(.x, n = min(2500, nrow(.x))))
```

The RDS file after first time cleaning can be found here:
```{r}
readRDS(here("filtered_data_copy.rds"))
```