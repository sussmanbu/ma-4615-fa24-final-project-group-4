---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)


This comes from the file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.


Initially, you will study _one dataset_ but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable.
Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.


## What makes a good data set?

* Data you are interested in and care about.
* Data where there are a lot of potential questions that you can explore.
* A data set that isn't completely cleaned already.
* Multiple sources for data that you can combine.
* Some type of time and/or location component.


## Where to keep data?


Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data


For small datasets (<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  "scripts/load_and_clean_data.R",
  echo = TRUE # Use echo=FALSE or omit it to avoid code output  
)
```
You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R). 

----

## Rubric: On this page

You will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Rrename variables and recode factors to make data more clear.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.
  

# Analysis and Cleaning of LAPD Crime Data (2020–Present)

# Data Documentation

## 1. Data Overview

The dataset contains reported crimes in Los Angeles from 2020 onward, sourced from the **LAPD**.

- **Source**: [LAPD Crime Data (2020–Present)](https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8/data_preview)
- **Attribution**: Derived from original crime reports typed on paper.
- **Purpose**:
  - Track crime patterns.
  - Allocate public safety resources.
  - Promote transparency and public accountability.
- **Challenges**:
  - Large dataset size (~987K rows, 28 variables).
  - Cleaning required to address duplicates, incomplete data, and usability for spatial/temporal analysis.

---

## 2. Variables in the Dataset

The dataset comprises 28 columns, described below:

| **Variable**        | **Description**                                                                                   | **Data Type**         |
|----------------------|---------------------------------------------------------------------------------------------------|------------------------|
| DR_NO               | Division of Records Number: Unique file identifier (YY+Area ID+5 digits).                        | Text                  |
| Date Rptd           | Date crime was reported (MM/DD/YYYY).                                                            | Timestamp             |
| DATE OCC            | Date crime occurred (MM/DD/YYYY).                                                                | Timestamp             |
| TIME OCC            | Time crime occurred (24-hour format).                                                            | Text                  |
| AREA                | LAPD’s 21 geographic areas (1–21).                                                               | Text                  |
| AREA NAME           | Name corresponding to the geographic area (e.g., 77th Street).                                   | Text                  |
| Rpt Dist No         | Sub-area code within a geographic area.                                                          | Text                  |
| Part 1-2            | Classification of crime as Part I or Part II.                                                    | Number                |
| Crm Cd              | Primary crime code.                                                                               | Text                  |
| Crm Cd Desc         | Description of the primary crime.                                                                | Text                  |
| Mocodes             | Suspect activities during the crime.                                                             | Text                  |
| Vict Age            | Age of the victim.                                                                               | Number                |
| Vict Sex            | Victim’s gender (M: Male, F: Female, X: Unknown).                                                | Text                  |
| Vict Descent        | Ethnic descent (e.g., H: Hispanic, W: White).                                                    | Text                  |
| Premis Cd           | Code for the type of location where the crime occurred.                                          | Number                |
| Premis Desc         | Description of the premise code.                                                                 | Text                  |
| Weapon Used Cd      | Code for the weapon used.                                                                        | Text                  |
| Weapon Desc         | Description of the weapon code.                                                                  | Text                  |
| Status              | Status of the case (e.g., IC: Incomplete).                                                       | Text                  |
| Status Desc         | Description of the status code.                                                                  | Text                  |
| Crm Cd 1–4          | Primary and additional crime codes.                                                              | Text                  |
| LOCATION            | Approximate address of the crime (anonymized).                                                  | Text                  |
| Cross Street        | Cross street near the crime location.                                                            | Text                  |
| LAT                 | Latitude of the crime location.                                                                  | Number                |
| LON                 | Longitude of the crime location.                                                                 | Number                |

---

## 3. Cleaning and Sampling

### Initial Dataset Loading
The dataset was loaded as a CSV file for easier manipulation. Here’s the R code for loading it:

```{r}
#| results: hide
#| warning: false
library(here)
library(readr)
data <- read_csv(here("your_file.csv"))
```
Then we start the data cleaning process. 

To reduce the dataset size (because it was too big previously) and balance representation across years, we:\

- Extracted the year from the DATE OCC column.\
- Filtered data for years 2020–2024.\
- Sampled up to 2,500 rows per year for efficiency.\

The following r chunk is the exact code we use by following the previous steps.
```{r}
# Convert the date column to year format
data$year <- format(as.Date(data$DATE.OCC, format = "%m/%d/%Y"), "%Y")
# Convert year to numeric
data$year <- as.numeric(data$year)  
# Filter for the years 2020–2024
filtered_years <- data %>%
  filter(year %in% 2020:2024)
# Split by year and sample up to 2,500 rows per year
sampled_data <- filtered_years %>%
  group_split(year) %>%
  map_df(~ slice_sample(.x, n = min(2500, nrow(.x))))
```

The RDS file after first time cleaning can look like this (some columns filtered out for readability):
```{r}
#| echo: False 
#| results: hide
datPage = readRDS(here("filtered_data_copy.rds"))
```
## Updated Dataset with Years Spread Across Multiple Years

| **DR_NO**   | **DATE.OCC**        | **Year** | **AREA.NAME** | **Crm.Cd** | **Crm.Cd.Desc**                        | **Premis.Desc**           | **LAT**     | **LON**      |
|-------------|---------------------|----------|---------------|------------|----------------------------------------|---------------------------|-------------|--------------|
| 200604124   | 01/03/2020          | 2020     | Hollywood     | 310        | BURGLARY                               | RESTAURANT/FAST FOOD      | 34.1000     | -118.3113    |
| 200218223   | 12/05/2019          | 2019     | Rampart       | 510        | VEHICLE - STOLEN                       | STREET                    | 34.0707     | -118.2693    |
| 210105387   | 02/02/2021          | 2021     | Central       | 649        | DOCUMENT FORGERY / STOLEN FELONY       | BANK                      | 34.0493     | -118.2582    |
| 201314683   | 07/29/2020          | 2020     | Newton        | 740        | VANDALISM - FELONY                     | VEHICLE, PASSENGER/TRUCK  | 33.9830     | -118.2783    |
| 201610483   | 06/28/2022          | 2022     | Foothill      | 354        | THEFT OF IDENTITY                      | SINGLE FAMILY DWELLING    | 34.2103     | -118.3747    |
| 200905047   | 01/25/2019          | 2019     | Van Nuys      | 745        | VANDALISM - MISDEMEANOR ($399 OR UNDER)| VEHICLE STORAGE LOT       | 34.1961     | -118.4487    |
| 200111368   | 05/05/2021          | 2021     | Central       | 940        | EXTORTION                              | MULTI-UNIT DWELLING       | 34.0433     | -118.2377    |
| 201508876   | 04/16/2022          | 2022     | N Hollywood   | 310        | BURGLARY                               | LAUNDROMAT                | 34.1649     | -118.3615    |

# Census Data Utilized in Crime Analysis
To enhance the depth of our crime analysis, we incorporated U.S. Census data to provide socioeconomic context for various geographic areas. The census data offers crucial insights into factors such as population size, median income, and racial composition, which are essential for understanding potential correlations between crime rates and socioeconomic variables.However, we wanted to look just at median income and take the average of the median income for an area. 

For this analysis, the census data was matched with specific crime locations, enabling us to explore whether crime rates differ significantly based on income levels. Median income was a particularly important variable, used to examine potential relationships between socioeconomic status and crime prevalence. Median income was used instead of average income due to high income areas which might have extreme outliers with certain individuals making a large amount of money. 

We ensured the census data was updated every decade, so this comes from 2020 but is mapped to the corresponding regions where crimes occurred. This allowed for a comprehensive analysis that moves beyond raw crime figures and considers underlying socioeconomic conditions, facilitating a more nuanced interpretation of trends and patterns.

By integrating census data, we aimed to uncover deeper insights, such as whether economically disadvantaged areas experience higher crime rates.


