---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)

This comes from the file `analysis.qmd`.

We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis.

The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc. 

While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points.
If you want you can link back to your blog posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. 
I don't expect this to be of publication quality but you should keep that aim in mind.
Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process.

### Example of loading data

```{r load libraries}
#| results: "hide"
#| echo: false
library(tidyverse)
library(sf)
library(tidycensus)
library(here)
library(viridis)
library(dplyr)
library(MASS)
library(tidyr)

# Load crime data and convert to spatial format
data <- readRDS(here("filtered_data_2.rds"))
crime_sf <- st_as_sf(data, coords = c("LON", "LAT"), crs = 4326)

# Load neighborhood boundaries
neighborhoods <- st_read(here("LA_Times_Neighborhood_Boundaries-shp/8494cd42-db48-4af1-a215-a2c8f61e96a22020328-1-621do0.x5yiu.shp")) %>%
  st_transform(st_crs(crime_sf))

# Join crime incidents with neighborhoods
crime_with_neighborhoods <- st_join(crime_sf, neighborhoods, join = st_within)
census_api_key("c2aebe6041f0c99e41a3458ed8d0b95ee3650fa4")

# Load and transform median income data
economic_data <- get_acs(
  geography = "block group",
  variables = c(median_income = "B19013_001"),
  state = "CA",
  county = "Los Angeles",
  year = 2020,
  geometry = TRUE
) %>% 
  st_transform(st_crs(crime_with_neighborhoods))

# Join income with neighborhood-level crime data
crime_with_income <- st_join(crime_with_neighborhoods, economic_data, join = st_within)

# Summarize crime data by neighborhood
crime_summary <- crime_with_income %>%
  group_by(GEOID) %>%
  summarize(
    total_crimes = n(),
    avg_income = mean(estimate, na.rm = TRUE),
    geometry = st_union(geometry)
  ) %>%
  filter(!is.na(avg_income)) %>%  # Remove rows with missing income
  st_as_sf()



lapd_boundary <- st_read(here("CityBoundaryofLosAngeles/geo_export_416eaeda-447a-4473-b003-37e2cad181ac.shp")) %>%
  st_transform(st_crs(crime_sf))  # Align CRS



crime_within_lapd <- crime_sf %>%
  filter(rowSums(st_within(geometry, st_geometry(lapd_boundary), sparse = FALSE)) > 0)

crime_with_income_lapd <- st_join(crime_within_lapd, economic_data, join = st_within)

economic_data_lapd <- st_intersection(economic_data, lapd_boundary)


# Update the crime summary
crime_summary_lapd <- crime_with_income_lapd %>%
  group_by(GEOID) %>%
  summarize(
    total_crimes = n(),
    avg_income = mean(estimate, na.rm = TRUE),
    geometry = st_union(geometry)
  ) %>%
  filter(!is.na(avg_income)) %>%  # Exclude rows with missing income
  st_as_sf()

crime_summary_race <- crime_with_income %>%
  group_by(GEOID, Descent_Description) %>%
  summarize(
    total_crimes = n(),  # Count crimes for each GEOID and Descent_Description group
    avg_median_income = mean(estimate, na.rm = TRUE)
  ) %>%
  pivot_wider(
    names_from = Descent_Description,
    values_from = total_crimes,
    values_fill = 0  # Fill missing values with 0 for crimes
  ) %>%
  rowwise() %>%
  mutate(
    total_crimes = sum(c_across(-c(GEOID, avg_median_income, geometry)), na.rm = TRUE)  # Sum only the crime columns
  ) %>%
  ungroup()

crime_summary_race <- crime_summary_race %>%
  rename_with(~ make.names(.), everything())

crime_summary_race_model <- crime_summary_race %>%
  filter(complete.cases(
    avg_median_income, Hispanic.Latin.Mexican, White, Black, Other, Other.Asian,
    Unknown, Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian,
    American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian
  ))


bivar_nb_model <- glm.nb(total_crimes ~ avg_median_income + Hispanic.Latin.Mexican + White + Black + Other + 
    Other.Asian + Unknown + Japanese + Korean + Filipino + Chinese + Hawaiian + 
    Asian.Indian + American.Indian.Alaskan.Native + Cambodian + Vietnamese + Laotian, data = crime_summary_race)

# Add predicted values to the filtered data
crime_summary_race_model <- crime_summary_race_model %>%
  mutate(predicted_crimes = predict(bivar_nb_model, type = "response"))

crime_race_long <- crime_summary_race_model %>%
  pivot_longer(
    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, 
             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, 
             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),
    names_to = "race",
    values_to = "crime_count"
  )


crime_race_long <- crime_summary_race %>%
  pivot_longer(
    cols = c(Hispanic.Latin.Mexican, White, Black, Other, Other.Asian, Unknown, 
             Japanese, Korean, Filipino, Chinese, Hawaiian, Asian.Indian, 
             American.Indian.Alaskan.Native, Cambodian, Vietnamese, Laotian),
    names_to = "race",
    values_to = "crime_count"
  ) %>%
  filter(!is.na(crime_count) & crime_count > 0) %>%
  mutate(avg_median_income = avg_median_income / 1000)



# Fit Poisson regression with pivoted data
poisson_model <- glm(
  crime_count ~ avg_median_income + race,
  family = poisson(link = "log"),
  data = crime_race_long
)
summary(poisson_model)

# Fit Negative Binomial regression with pivoted data
crime_race_long <- crime_race_long %>%
  filter(complete.cases(avg_median_income, race, crime_count))

nb_model <- glm.nb(
  crime_count ~ avg_median_income * race,
  data = crime_race_long
)

crime_race_long <- crime_race_long %>%
  mutate(predicted_crimes = predict(nb_model, newdata = crime_race_long, type = "response"))






```



```{r}
#| echo: false
crime_with_income <- st_join(crime_with_neighborhoods, economic_data, join = st_within)
median_income_by_neighborhood <- crime_with_income %>%
  group_by(AREA.NAME) %>%
  summarize(
    median_income = median(estimate, na.rm = TRUE),
    total_crimes = n(),  # Optional: Include total crimes
    geometry = st_union(geometry)
  ) %>%
  filter(!is.na(median_income)) %>% 
  st_as_sf()
ggplot(median_income_by_neighborhood, aes(x = median_income, y = total_crimes)) +
  geom_point(aes(color = median_income, size = total_crimes), alpha = 0.7) +
  scale_color_viridis_c(option = "plasma", name = "Median Income") +
  labs(
    title = "Relationship Between Median Income and Total Crimes",
    x = "Median Income (USD)",
    y = "Total Crimes",
    caption = "Source: ACS 2020 and Crime Data"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```


## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.

If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
    -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description of the relevant data.
    
    
    
# What Motivated Our Data Analysis:
Crime is a pervasive issue that affects communities differently based on various socioeconomic, demographic, and geographic factors. Understanding what drives crime frequency is crucial for developing effective policies and interventions to promote equity and public safety.

Our data analysis was motivated by the desire to uncover relationships between crime and the following key factors:

**Economic Status** (e.g., median income) \
**Demographics** (e.g., racial composition) \
**Location** (e.g., neighborhood characteristics) \
\



# Variables of Interest

The primary variables we analyzed included:

**Crime Frequency:** The total number of reported crimes in a neighborhood. \
**Economic Data:** Median household income from the American Community Survey (ACS). \
**Neighborhood Boundaries:** Spatial boundaries defined by the LA Times. \
**Location Data:** Geospatial coordinates (latitude and longitude) for reported crime incidents. \
#Add to here as necessary \
To explore these variables, we integrated datasets from LAPD crime reports, U.S. Census economic data, and geospatial shapefiles for Los Angeles neighborhoods. This allowed us to analyze both spatial and statistical relationships. \
#Add links here

# Key Questions
**Economic Status and Crime:** Does a neighborhood's median income influence the number of crimes reported? \
**Geographic Crime Hotspots:** Are there specific areas with persistently high crime rates? \
**Disparities in Crime Impact:** How do demographic or socioeconomic disparities correlate with crime?
#Add here as necessary \

# Modeling and Inference
## Motivation for the Model
We aim to understand the relationship between median income and crime, considering racial demographics as a moderating factor. This requires modeling total crimes as a function of average median income and racial groupings. To account for overdispersion in crime count data, we employ both Poisson regression and Negative Binomial regression frameworks.

## Model Selection
**Why Poisson and Negative Binomial Models?**

Crime counts are discrete and non-negative, making Poisson regression an intuitive starting point. \
Overdispersion (variance exceeding the mean) was evident, prompting the use of a Negative Binomial regression model.
Predictors Considered

**Median income**: Hypothesized to influence crime rates inversely. \
**Race**: Used as categorical predictors to examine demographic disparities in crime incidence. \
**Interaction terms**: Incorporated to explore how income effects vary across racial groups. \

## Modeling Steps

### Poisson regression model:
```{r Poisson}
#| echo: False 
#| results: "hide"
poisson_model <- glm(crime_count ~ avg_median_income + race, family = poisson(link = "log"), data = crime_race_long)
summary(poisson_model)
```
#### Coefficients:
- **avg_median_income**: -0.0019821 (p < 0.001, significant)
- **raceBlack**: 0.6206 (p = 0.007, significant)
- **raceHispanic.Latin.Mexican**: 0.8102 (p < 0.001, significant)
- **raceUnknown**: 0.7717 (p < 0.001, significant)
- **raceWhite**: 0.6654 (p = 0.004, significant)

#### Model Metrics:
- Null deviance: 6300.2 on 5907 degrees of freedom
- Residual deviance: 5790.0 on 5891 degrees of freedom
- AIC: 20106

**Key Insights:**

avg_median_income is negatively associated with crime counts and is statistically significant. \
Certain racial categories (raceHispanic.Latin.Mexican, raceUnknown, raceWhite, and raceBlack) are positively associated with higher crime counts and are statistically significant. \

### Negative Binomial regression model:
```{r Neg. Binomial}
#| echo: False 
#| results: "hide"
nb_model <- glm.nb(crime_count ~ avg_median_income * race, data = crime_race_long)
summary(nb_model)
```
#### Coefficients:
- **raceHispanic.Latin.Mexican**: 1.0450 (p = 0.013, significant)
- **raceUnknown**: 0.7903 (p = 0.061, marginally significant)
- **raceBlack**: 0.7862 (p = 0.064, marginally significant)

#### Model Metrics:
- Null deviance: 4800.1 on 5907 degrees of freedom
- Residual deviance: 4305.8 on 5877 degrees of freedom
- AIC: 19758
- Dispersion parameter: Theta = 9.252 (SE = 0.706)

**Key Insights:**

raceHispanic.Latin.Mexican remains statistically significant and positively associated with crime counts. \
The inclusion of overdispersion (via the Negative Binomial model) leads to better model fit metrics (lower AIC and deviance) compared to the Poisson regression. \
We evaluate model fit using AIC and residual diagnostics, concluding the Negative Binomial model better accounts for data characteristics. \

### Why Negative Binomial Was Better
**Dispersion Issue:** The Poisson model assumes that the mean and variance of the dependent variable (crime counts) are equal. However, the data exhibits overdispersion (variance > mean), as indicated by the high residual deviance in the Poisson model. \
**Model Fit:** The Negative Binomial regression explicitly accounts for overdispersion with a dispersion parameter (Theta = 9.252), leading to improved model fit (lower AIC: 19758 vs. 20106 and reduced deviance). \
**Interpretation:** While both models identify significant predictors (e.g., raceHispanic.Latin.Mexican), the Negative Binomial model provides more reliable estimates due to its ability to handle overdispersed data.


## Predicted Trends
Using the model, we predict crime counts across income ranges for each racial group:
```{r Predicted Trends}
#| echo: False

grid <- expand.grid(
  avg_median_income = seq(min(crime_race_long$avg_median_income), max(crime_race_long$avg_median_income), length.out = 100),
  race = unique(crime_race_long$race)
)
grid$predicted_crimes <- predict(nb_model, newdata = grid, type = "response")
```
### Visualization
```{r}
#| echo: False
significant_races <- summary(nb_model)$coefficients %>%
  as.data.frame() %>%
  rownames_to_column("race") %>%
  filter(grepl("race", race)) %>%  # Keep only race-related coefficients
  mutate(race = gsub("race", "", race)) %>%  # Clean up race names
  filter(`Pr(>|z|)` <= 0.8) %>%  # Keep only races with p-values <= 0.7
  filter(!race %in% c("Unknown", "Other")) %>%  # Exclude "Unknown" and "Other"
  pull(race)

# Filter the data to include only significant races
filtered_crime_race_long <- crime_race_long %>%
  filter(race %in% significant_races)

# Plot the ggplot
ggplot(filtered_crime_race_long, aes(x = avg_median_income, y = predicted_crimes, color = race)) +
  geom_line() +
  facet_wrap(~race, scales = "free_y") +  # Facet by race, allowing y-axis to vary by panel
  labs(
    title = "Predicted Crime Counts vs Median Income by Race (Facet by Race)",
    x = "Average Median Income (in $1,000s)",
    y = "Predicted Crime Counts"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 10),  # Adjust facet labels
    legend.position = "none"              # Hide legend if redundant
  )
```
This figure illustrates how income impacts predicted crime counts, highlighting disparities among racial groups.\
This also removed any race with a P-Value over 0.8 and any races like "unknown" and "other" because that doesn't give much information.

### Uncertainty and Limitations
**1. Uncertainty in Estimates:**

- Wide confidence intervals for certain racial group coefficients suggest variability in observed patterns.\
- Limited data for smaller racial groups (e.g., Laotian, Cambodian) may result in unreliable estimates.\

**2.Model Assumptions:**

- Both Poisson and Negative Binomial models assume log-linear relationships, which may oversimplify real-world dynamics.\
- Spatial dependencies (e.g., crime clustering in neighborhoods) are not accounted for, potentially biasing results.\

**3. Data Limitations:**

- The dataset has missing values for some predictors, leading to excluded observations.\
- Aggregation by neighborhood may obscure within-neighborhood variability in income and crime rates.\

### Future Directions

- Integrate additional predictors like unemployment rates, educational attainment, or policing density to refine insights.\
- Perform robustness checks by using alternative categorizations for racial groups or income brackets.\




